{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IzR-XiGUXhx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "e107e6b3-a085-4363-dd3d-29c89cf2a6d7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QjrT3NPVTwn"
      },
      "source": [
        "mail = pd.read_csv(\"drive/My Drive/Colab Notebooks/news_summary_more1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml-yXyxlVopV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "4569a060-afa1-4c63-c023-3925333fd978"
      },
      "source": [
        "mail.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1  Delhi techie wins free food from Swiggy for on...  Kunal Shah's credit card bill payment platform...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  New Zealand defeated India by 8 wickets in the...\n",
              "3  Aegon life iTerm insurance plan helps customer...  With Aegon Life iTerm Insurance plan, customer...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  Speaking about the sexual harassment allegatio..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-HklmdWVy6Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e32daeb1-c098-4a52-a684-4afa71542846"
      },
      "source": [
        "mail.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "headlines    0\n",
              "text         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhTNEnjJV4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6b2394d-22b0-431d-fca4-beeea29b53ce"
      },
      "source": [
        "mail.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29450, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCwIBH-7V-6E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "a61baee2-50d3-47f6-9a06-305ebbf1ae49"
      },
      "source": [
        "print(mail.headlines.describe())\n",
        "print(mail.text.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count                                                 29450\n",
            "unique                                                29448\n",
            "top       Indian Oil looking for annual deal to buy crud...\n",
            "freq                                                      2\n",
            "Name: headlines, dtype: object\n",
            "count                                                 29450\n",
            "unique                                                29449\n",
            "top       Safeguard yourself against life's unpleasant s...\n",
            "freq                                                      2\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMaY8TfoWH0I"
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJg_gTUrWPlP"
      },
      "source": [
        "def clean_text(text, remove_stopwords):\n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jED5DgeWXGd"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOdv9o-cWcU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "8cf37392-831c-44c3-ec83-87e7ff33a9a8"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieWItqQJWf1S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "483eb423-e304-49e3-8c63-9702df31c794"
      },
      "source": [
        "clean_summaries = []\n",
        "for headlines in mail.headlines:\n",
        "    clean_summaries.append(clean_text(str(headlines), remove_stopwords=False))\n",
        "print(\"Headlines are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in mail.text:\n",
        "    clean_texts.append(clean_text(str(text), remove_stopwords=True))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Headlines are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S08rps2_WsGd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "3078bce2-cd78-4436-bd87-9debfc66e69b"
      },
      "source": [
        "for i in range(0,5):\n",
        "    print(\"Text: \", clean_texts[i])\n",
        "    print(\"Headlines: \",clean_summaries[i])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  saurav kant alumnus upgrad iiit b pg program machine learning artificial intelligence sr systems engineer infosys almost 5 years work experience program upgrad 360 degree career support helped transition data scientist tech mahindra 90 salary hike upgrad online power learning powered 3 lakh careers\n",
            "Headlines:  upgrad learner switches to career in ml   al with 90  salary hike\n",
            "\n",
            "Text:  kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending 2000 cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult fit\n",
            "Headlines:  delhi techie wins free food from swiggy for one year on cred\n",
            "\n",
            "Text:  new zealand defeated india 8 wickets fourth odi hamilton thursday win first match five match odi series india lost international match rohit sharma captaincy 12 consecutive victories dating back march 2018 match witnessed india getting 92 seventh lowest total odi cricket history\n",
            "Headlines:  new zealand end rohit sharma led india s 12 match winning streak\n",
            "\n",
            "Text:  aegon life iterm insurance plan customers enjoy tax benefits premiums paid save â¹46 800^ taxes plan provides life cover age 100 years also customers options insure critical illnesses disability accidental death benefit rider life cover age 80 years\n",
            "Headlines:  aegon life iterm insurance plan helps customers save tax\n",
            "\n",
            "Text:  speaking sexual harassment allegations rajkumar hirani sonam kapoor said known hirani many years true metoo movement get derailed metoo movement always believe woman case need reserve judgment added hirani accused assistant worked sanju\n",
            "Headlines:  have known hirani for yrs  what if metoo claims are not true  sonam\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC1lB3QUW6CZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef2d7271-660c-4791-ef5f-6323ac2a4fa0"
      },
      "source": [
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1\n",
        "                \n",
        "'''#Cell 11: Find the number of times each word was used and the size of the vocabulary'''\n",
        "word_counts = {}\n",
        "\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 48721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU8U2_z9W9E5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "becd0271-cc5c-45d6-8e70-a58eb87658b0"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('drive/My Drive/Colab Notebooks/numberbatch-en.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 516783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8C9m_MjXPHS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "84f02e2b-6fc9-4978-b7c3-cb80aed9cd57"
      },
      "source": [
        "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from CN: 886\n",
            "Percent of words that are missing from vocabulary: 1.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5iXSFZLX41P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "218b2f0f-b4ab-4215-83de-8a5e42f0e78e"
      },
      "source": [
        "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
        "\n",
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 48721\n",
            "Number of words we will use: 32857\n",
            "Percent of words we will use: 67.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4ReIKhYDt1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2543d611-1d0c-4731-8f3c-12e2a428f8fe"
      },
      "source": [
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        #embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLPDkbAVYLK9"
      },
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.If word is not in vocab_to_int, use UNK's integer.Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qACt2bXsYURH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ddc3f06c-58a3-4562-8c0a-8449592f04cf"
      },
      "source": [
        "# Apply convert_to_ints to clean_summaries and clean_texts\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 1403785\n",
            "Total number of UNKs in headlines: 42493\n",
            "Percent of words that are UNK: 3.0300000000000002%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wllTA8nDYlDM"
      },
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHPFa6YfYsJR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "fd890a9b-329f-42f1-a3a1-e5eddde2e579"
      },
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "             counts\n",
            "count  29450.000000\n",
            "mean      10.496129\n",
            "std        1.608009\n",
            "min        1.000000\n",
            "25%        9.000000\n",
            "50%       10.000000\n",
            "75%       12.000000\n",
            "max       18.000000\n",
            "\n",
            "Texts:\n",
            "             counts\n",
            "count  29450.000000\n",
            "mean      38.170594\n",
            "std        4.452644\n",
            "min        2.000000\n",
            "25%       35.000000\n",
            "50%       38.000000\n",
            "75%       41.000000\n",
            "max       61.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoRPRhY8YxeM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b32db584-cd73-45e3-fa4f-c1a99157b0aa"
      },
      "source": [
        "# Inspect the length of texts\n",
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44.0\n",
            "46.0\n",
            "49.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoVWQgfGY6Gx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4b118b60-468e-4b57-ff79-3353dacc48f9"
      },
      "source": [
        "# Inspect the length of summaries\n",
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13.0\n",
            "13.0\n",
            "14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_rXBs8AY-0-"
      },
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkT-_bfrZEZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bb5b7a2d-8b9e-4117-d9da-09cef11de17b"
      },
      "source": [
        "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
        "# Limit the length of summaries and texts based on the min and max ranges.\n",
        "# Remove reviews that include too many UNKs\n",
        "\n",
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 84\n",
        "max_summary_length = 13\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17696\n",
            "17696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oWKbRcMZ8PU"
      },
      "source": [
        "### BUILDING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuAeGWElZzV6"
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAK_EX2IaK_T"
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MIwT50taR-S"
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbfcVU7-aX8Q"
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mTpoMqOaeYx"
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XqFmvFsajr-"
      },
      "source": [
        "#output_units=12\n",
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp_5JdZebNet"
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSMzMms-bVDc"
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1pB0VzDbZkW"
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt8t9ewPbj12"
      },
      "source": [
        "## TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euEgrpjbbiti"
      },
      "source": [
        "epochs = 25\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.01\n",
        "keep_probability = 0.75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mliH5P8hbwz0"
      },
      "source": [
        "# GRAPH BUILDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVtc3TV2btxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "e175fffe-6d32-4f9a-f99d-b098994777f5"
      },
      "source": [
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-28-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-28-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AD3U7fpJnex"
      },
      "source": [
        "# Training the graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW4HvS-5JMmX"
      },
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 5 # Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 3 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "\n",
        "checkpoint = \"./best_model.ckpt\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dde5K0nRQfeu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3b36739-549e-4b56-d680-1ee91fb16afc"
      },
      "source": [
        "print(\"Training will Strat now.\")\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            \n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries, sorted_texts, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "\n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "        saver = tf.train.Saver() \n",
        "        saver.save(sess, checkpoint)\n",
        "\n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break\n",
        "\n",
        "print(\"Model Trained\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training will Strat now.\n",
            "Epoch   1/25 Batch    5/276 - Loss: 16.260, Seconds: 0.55\n",
            "Epoch   1/25 Batch   10/276 - Loss:  7.545, Seconds: 0.60\n",
            "Epoch   1/25 Batch   15/276 - Loss:  6.965, Seconds: 0.59\n",
            "Epoch   1/25 Batch   20/276 - Loss:  6.761, Seconds: 0.59\n",
            "Epoch   1/25 Batch   25/276 - Loss:  6.780, Seconds: 0.62\n",
            "Epoch   1/25 Batch   30/276 - Loss:  6.591, Seconds: 0.62\n",
            "Epoch   1/25 Batch   35/276 - Loss:  6.586, Seconds: 0.62\n",
            "Epoch   1/25 Batch   40/276 - Loss:  6.535, Seconds: 0.66\n",
            "Epoch   1/25 Batch   45/276 - Loss:  6.371, Seconds: 0.63\n",
            "Epoch   1/25 Batch   50/276 - Loss:  6.530, Seconds: 0.67\n",
            "Epoch   1/25 Batch   55/276 - Loss:  6.514, Seconds: 0.64\n",
            "Epoch   1/25 Batch   60/276 - Loss:  6.434, Seconds: 0.60\n",
            "Epoch   1/25 Batch   65/276 - Loss:  6.283, Seconds: 0.64\n",
            "Epoch   1/25 Batch   70/276 - Loss:  6.368, Seconds: 0.67\n",
            "Epoch   1/25 Batch   75/276 - Loss:  6.303, Seconds: 0.64\n",
            "Epoch   1/25 Batch   80/276 - Loss:  6.276, Seconds: 0.68\n",
            "Epoch   1/25 Batch   85/276 - Loss:  6.231, Seconds: 0.66\n",
            "Epoch   1/25 Batch   90/276 - Loss:  6.160, Seconds: 0.64\n",
            "Average loss for this update: 7.071\n",
            "New Record!\n",
            "Epoch   1/25 Batch   95/276 - Loss:  6.300, Seconds: 0.69\n",
            "Epoch   1/25 Batch  100/276 - Loss:  6.296, Seconds: 0.65\n",
            "Epoch   1/25 Batch  105/276 - Loss:  6.202, Seconds: 0.65\n",
            "Epoch   1/25 Batch  110/276 - Loss:  6.225, Seconds: 0.66\n",
            "Epoch   1/25 Batch  115/276 - Loss:  6.049, Seconds: 0.67\n",
            "Epoch   1/25 Batch  120/276 - Loss:  6.076, Seconds: 0.66\n",
            "Epoch   1/25 Batch  125/276 - Loss:  6.151, Seconds: 0.69\n",
            "Epoch   1/25 Batch  130/276 - Loss:  6.123, Seconds: 0.67\n",
            "Epoch   1/25 Batch  135/276 - Loss:  6.046, Seconds: 0.68\n",
            "Epoch   1/25 Batch  140/276 - Loss:  5.968, Seconds: 0.70\n",
            "Epoch   1/25 Batch  145/276 - Loss:  5.930, Seconds: 0.70\n",
            "Epoch   1/25 Batch  150/276 - Loss:  6.204, Seconds: 0.70\n",
            "Epoch   1/25 Batch  155/276 - Loss:  6.012, Seconds: 0.73\n",
            "Epoch   1/25 Batch  160/276 - Loss:  5.963, Seconds: 0.70\n",
            "Epoch   1/25 Batch  165/276 - Loss:  5.809, Seconds: 0.69\n",
            "Epoch   1/25 Batch  170/276 - Loss:  5.807, Seconds: 0.73\n",
            "Epoch   1/25 Batch  175/276 - Loss:  6.097, Seconds: 0.71\n",
            "Epoch   1/25 Batch  180/276 - Loss:  5.915, Seconds: 0.70\n",
            "Average loss for this update: 6.059\n",
            "New Record!\n",
            "Epoch   1/25 Batch  185/276 - Loss:  5.904, Seconds: 0.72\n",
            "Epoch   1/25 Batch  190/276 - Loss:  5.820, Seconds: 0.69\n",
            "Epoch   1/25 Batch  195/276 - Loss:  5.750, Seconds: 0.76\n",
            "Epoch   1/25 Batch  200/276 - Loss:  5.840, Seconds: 0.71\n",
            "Epoch   1/25 Batch  205/276 - Loss:  5.789, Seconds: 0.70\n",
            "Epoch   1/25 Batch  210/276 - Loss:  5.804, Seconds: 0.74\n",
            "Epoch   1/25 Batch  215/276 - Loss:  5.672, Seconds: 0.72\n",
            "Epoch   1/25 Batch  220/276 - Loss:  5.746, Seconds: 0.73\n",
            "Epoch   1/25 Batch  225/276 - Loss:  5.609, Seconds: 0.73\n",
            "Epoch   1/25 Batch  230/276 - Loss:  5.629, Seconds: 0.76\n",
            "Epoch   1/25 Batch  235/276 - Loss:  5.681, Seconds: 0.74\n",
            "Epoch   1/25 Batch  240/276 - Loss:  5.618, Seconds: 0.71\n",
            "Epoch   1/25 Batch  245/276 - Loss:  5.597, Seconds: 0.73\n",
            "Epoch   1/25 Batch  250/276 - Loss:  5.691, Seconds: 0.74\n",
            "Epoch   1/25 Batch  255/276 - Loss:  5.493, Seconds: 0.77\n",
            "Epoch   1/25 Batch  260/276 - Loss:  5.529, Seconds: 0.75\n",
            "Epoch   1/25 Batch  265/276 - Loss:  5.571, Seconds: 0.78\n",
            "Epoch   1/25 Batch  270/276 - Loss:  5.540, Seconds: 0.80\n",
            "Average loss for this update: 5.675\n",
            "New Record!\n",
            "Epoch   1/25 Batch  275/276 - Loss:  5.537, Seconds: 0.87\n",
            "Epoch   2/25 Batch    5/276 - Loss:  6.683, Seconds: 0.57\n",
            "Epoch   2/25 Batch   10/276 - Loss:  5.431, Seconds: 0.58\n",
            "Epoch   2/25 Batch   15/276 - Loss:  5.339, Seconds: 0.58\n",
            "Epoch   2/25 Batch   20/276 - Loss:  5.291, Seconds: 0.61\n",
            "Epoch   2/25 Batch   25/276 - Loss:  5.236, Seconds: 0.68\n",
            "Epoch   2/25 Batch   30/276 - Loss:  5.201, Seconds: 0.63\n",
            "Epoch   2/25 Batch   35/276 - Loss:  5.171, Seconds: 0.63\n",
            "Epoch   2/25 Batch   40/276 - Loss:  5.101, Seconds: 0.62\n",
            "Epoch   2/25 Batch   45/276 - Loss:  5.080, Seconds: 0.65\n",
            "Epoch   2/25 Batch   50/276 - Loss:  5.205, Seconds: 0.63\n",
            "Epoch   2/25 Batch   55/276 - Loss:  5.095, Seconds: 0.64\n",
            "Epoch   2/25 Batch   60/276 - Loss:  5.114, Seconds: 0.61\n",
            "Epoch   2/25 Batch   65/276 - Loss:  4.987, Seconds: 0.62\n",
            "Epoch   2/25 Batch   70/276 - Loss:  5.014, Seconds: 0.64\n",
            "Epoch   2/25 Batch   75/276 - Loss:  5.007, Seconds: 0.64\n",
            "Epoch   2/25 Batch   80/276 - Loss:  4.936, Seconds: 0.67\n",
            "Epoch   2/25 Batch   85/276 - Loss:  5.008, Seconds: 0.63\n",
            "Epoch   2/25 Batch   90/276 - Loss:  4.882, Seconds: 0.66\n",
            "Average loss for this update: 5.205\n",
            "New Record!\n",
            "Epoch   2/25 Batch   95/276 - Loss:  4.998, Seconds: 0.68\n",
            "Epoch   2/25 Batch  100/276 - Loss:  4.985, Seconds: 0.67\n",
            "Epoch   2/25 Batch  105/276 - Loss:  4.974, Seconds: 0.66\n",
            "Epoch   2/25 Batch  110/276 - Loss:  5.015, Seconds: 0.67\n",
            "Epoch   2/25 Batch  115/276 - Loss:  4.880, Seconds: 0.67\n",
            "Epoch   2/25 Batch  120/276 - Loss:  4.838, Seconds: 0.67\n",
            "Epoch   2/25 Batch  125/276 - Loss:  4.933, Seconds: 0.67\n",
            "Epoch   2/25 Batch  130/276 - Loss:  4.872, Seconds: 0.69\n",
            "Epoch   2/25 Batch  135/276 - Loss:  4.831, Seconds: 0.67\n",
            "Epoch   2/25 Batch  140/276 - Loss:  4.808, Seconds: 0.75\n",
            "Epoch   2/25 Batch  145/276 - Loss:  4.773, Seconds: 0.68\n",
            "Epoch   2/25 Batch  150/276 - Loss:  5.079, Seconds: 0.69\n",
            "Epoch   2/25 Batch  155/276 - Loss:  4.837, Seconds: 0.68\n",
            "Epoch   2/25 Batch  160/276 - Loss:  4.801, Seconds: 0.70\n",
            "Epoch   2/25 Batch  165/276 - Loss:  4.744, Seconds: 0.69\n",
            "Epoch   2/25 Batch  170/276 - Loss:  4.757, Seconds: 0.76\n",
            "Epoch   2/25 Batch  175/276 - Loss:  4.956, Seconds: 0.70\n",
            "Epoch   2/25 Batch  180/276 - Loss:  4.819, Seconds: 0.72\n",
            "Average loss for this update: 4.881\n",
            "New Record!\n",
            "Epoch   2/25 Batch  185/276 - Loss:  4.835, Seconds: 0.74\n",
            "Epoch   2/25 Batch  190/276 - Loss:  4.786, Seconds: 0.68\n",
            "Epoch   2/25 Batch  195/276 - Loss:  4.680, Seconds: 0.77\n",
            "Epoch   2/25 Batch  200/276 - Loss:  4.788, Seconds: 0.73\n",
            "Epoch   2/25 Batch  205/276 - Loss:  4.760, Seconds: 0.73\n",
            "Epoch   2/25 Batch  210/276 - Loss:  4.697, Seconds: 0.70\n",
            "Epoch   2/25 Batch  215/276 - Loss:  4.748, Seconds: 0.72\n",
            "Epoch   2/25 Batch  220/276 - Loss:  4.699, Seconds: 0.71\n",
            "Epoch   2/25 Batch  225/276 - Loss:  4.663, Seconds: 0.74\n",
            "Epoch   2/25 Batch  230/276 - Loss:  4.675, Seconds: 0.71\n",
            "Epoch   2/25 Batch  235/276 - Loss:  4.730, Seconds: 0.74\n",
            "Epoch   2/25 Batch  240/276 - Loss:  4.610, Seconds: 0.72\n",
            "Epoch   2/25 Batch  245/276 - Loss:  4.667, Seconds: 0.74\n",
            "Epoch   2/25 Batch  250/276 - Loss:  4.704, Seconds: 0.80\n",
            "Epoch   2/25 Batch  255/276 - Loss:  4.546, Seconds: 0.73\n",
            "Epoch   2/25 Batch  260/276 - Loss:  4.582, Seconds: 0.75\n",
            "Epoch   2/25 Batch  265/276 - Loss:  4.655, Seconds: 0.79\n",
            "Epoch   2/25 Batch  270/276 - Loss:  4.595, Seconds: 0.79\n",
            "Average loss for this update: 4.686\n",
            "New Record!\n",
            "Epoch   2/25 Batch  275/276 - Loss:  4.597, Seconds: 0.89\n",
            "Epoch   3/25 Batch    5/276 - Loss:  5.910, Seconds: 0.59\n",
            "Epoch   3/25 Batch   10/276 - Loss:  4.705, Seconds: 0.61\n",
            "Epoch   3/25 Batch   15/276 - Loss:  4.677, Seconds: 0.63\n",
            "Epoch   3/25 Batch   20/276 - Loss:  4.617, Seconds: 0.62\n",
            "Epoch   3/25 Batch   25/276 - Loss:  4.561, Seconds: 0.65\n",
            "Epoch   3/25 Batch   30/276 - Loss:  4.590, Seconds: 0.65\n",
            "Epoch   3/25 Batch   35/276 - Loss:  4.552, Seconds: 0.63\n",
            "Epoch   3/25 Batch   40/276 - Loss:  4.425, Seconds: 0.69\n",
            "Epoch   3/25 Batch   45/276 - Loss:  4.429, Seconds: 0.65\n",
            "Epoch   3/25 Batch   50/276 - Loss:  4.530, Seconds: 0.63\n",
            "Epoch   3/25 Batch   55/276 - Loss:  4.383, Seconds: 0.68\n",
            "Epoch   3/25 Batch   60/276 - Loss:  4.460, Seconds: 0.63\n",
            "Epoch   3/25 Batch   65/276 - Loss:  4.302, Seconds: 0.65\n",
            "Epoch   3/25 Batch   70/276 - Loss:  4.381, Seconds: 0.71\n",
            "Epoch   3/25 Batch   75/276 - Loss:  4.384, Seconds: 0.67\n",
            "Epoch   3/25 Batch   80/276 - Loss:  4.321, Seconds: 0.66\n",
            "Epoch   3/25 Batch   85/276 - Loss:  4.294, Seconds: 0.66\n",
            "Epoch   3/25 Batch   90/276 - Loss:  4.310, Seconds: 0.67\n",
            "Average loss for this update: 4.541\n",
            "New Record!\n",
            "Epoch   3/25 Batch   95/276 - Loss:  4.358, Seconds: 0.67\n",
            "Epoch   3/25 Batch  100/276 - Loss:  4.360, Seconds: 0.67\n",
            "Epoch   3/25 Batch  105/276 - Loss:  4.380, Seconds: 0.88\n",
            "Epoch   3/25 Batch  110/276 - Loss:  4.418, Seconds: 0.65\n",
            "Epoch   3/25 Batch  115/276 - Loss:  4.270, Seconds: 0.70\n",
            "Epoch   3/25 Batch  120/276 - Loss:  4.242, Seconds: 0.73\n",
            "Epoch   3/25 Batch  125/276 - Loss:  4.301, Seconds: 0.69\n",
            "Epoch   3/25 Batch  130/276 - Loss:  4.249, Seconds: 0.72\n",
            "Epoch   3/25 Batch  135/276 - Loss:  4.300, Seconds: 0.66\n",
            "Epoch   3/25 Batch  140/276 - Loss:  4.169, Seconds: 0.67\n",
            "Epoch   3/25 Batch  145/276 - Loss:  4.236, Seconds: 0.73\n",
            "Epoch   3/25 Batch  150/276 - Loss:  4.418, Seconds: 0.69\n",
            "Epoch   3/25 Batch  155/276 - Loss:  4.214, Seconds: 0.70\n",
            "Epoch   3/25 Batch  160/276 - Loss:  4.185, Seconds: 0.72\n",
            "Epoch   3/25 Batch  165/276 - Loss:  4.184, Seconds: 0.70\n",
            "Epoch   3/25 Batch  170/276 - Loss:  4.172, Seconds: 0.77\n",
            "Epoch   3/25 Batch  175/276 - Loss:  4.307, Seconds: 0.74\n",
            "Epoch   3/25 Batch  180/276 - Loss:  4.265, Seconds: 0.70\n",
            "Average loss for this update: 4.278\n",
            "New Record!\n",
            "Epoch   3/25 Batch  185/276 - Loss:  4.234, Seconds: 0.72\n",
            "Epoch   3/25 Batch  190/276 - Loss:  4.136, Seconds: 0.68\n",
            "Epoch   3/25 Batch  195/276 - Loss:  4.134, Seconds: 0.74\n",
            "Epoch   3/25 Batch  200/276 - Loss:  4.174, Seconds: 0.74\n",
            "Epoch   3/25 Batch  205/276 - Loss:  4.212, Seconds: 0.95\n",
            "Epoch   3/25 Batch  210/276 - Loss:  4.114, Seconds: 0.75\n",
            "Epoch   3/25 Batch  215/276 - Loss:  4.198, Seconds: 0.75\n",
            "Epoch   3/25 Batch  220/276 - Loss:  4.116, Seconds: 0.82\n",
            "Epoch   3/25 Batch  225/276 - Loss:  4.107, Seconds: 0.71\n",
            "Epoch   3/25 Batch  230/276 - Loss:  4.044, Seconds: 0.73\n",
            "Epoch   3/25 Batch  235/276 - Loss:  4.217, Seconds: 0.76\n",
            "Epoch   3/25 Batch  240/276 - Loss:  4.052, Seconds: 0.76\n",
            "Epoch   3/25 Batch  245/276 - Loss:  4.091, Seconds: 0.75\n",
            "Epoch   3/25 Batch  250/276 - Loss:  4.126, Seconds: 0.75\n",
            "Epoch   3/25 Batch  255/276 - Loss:  3.988, Seconds: 0.79\n",
            "Epoch   3/25 Batch  260/276 - Loss:  3.997, Seconds: 0.83\n",
            "Epoch   3/25 Batch  265/276 - Loss:  4.029, Seconds: 0.79\n",
            "Epoch   3/25 Batch  270/276 - Loss:  4.042, Seconds: 0.82\n",
            "Average loss for this update: 4.111\n",
            "New Record!\n",
            "Epoch   3/25 Batch  275/276 - Loss:  4.074, Seconds: 0.93\n",
            "Epoch   4/25 Batch    5/276 - Loss:  5.253, Seconds: 0.58\n",
            "Epoch   4/25 Batch   10/276 - Loss:  4.220, Seconds: 0.59\n",
            "Epoch   4/25 Batch   15/276 - Loss:  4.219, Seconds: 0.59\n",
            "Epoch   4/25 Batch   20/276 - Loss:  4.162, Seconds: 0.63\n",
            "Epoch   4/25 Batch   25/276 - Loss:  4.100, Seconds: 0.64\n",
            "Epoch   4/25 Batch   30/276 - Loss:  4.108, Seconds: 0.72\n",
            "Epoch   4/25 Batch   35/276 - Loss:  4.047, Seconds: 0.65\n",
            "Epoch   4/25 Batch   40/276 - Loss:  3.989, Seconds: 0.64\n",
            "Epoch   4/25 Batch   45/276 - Loss:  3.902, Seconds: 0.65\n",
            "Epoch   4/25 Batch   50/276 - Loss:  4.044, Seconds: 0.69\n",
            "Epoch   4/25 Batch   55/276 - Loss:  4.021, Seconds: 0.65\n",
            "Epoch   4/25 Batch   60/276 - Loss:  3.919, Seconds: 0.63\n",
            "Epoch   4/25 Batch   65/276 - Loss:  3.917, Seconds: 0.63\n",
            "Epoch   4/25 Batch   70/276 - Loss:  3.943, Seconds: 0.65\n",
            "Epoch   4/25 Batch   75/276 - Loss:  3.975, Seconds: 0.67\n",
            "Epoch   4/25 Batch   80/276 - Loss:  3.816, Seconds: 0.66\n",
            "Epoch   4/25 Batch   85/276 - Loss:  3.877, Seconds: 0.65\n",
            "Epoch   4/25 Batch   90/276 - Loss:  3.855, Seconds: 0.73\n",
            "Average loss for this update: 4.072\n",
            "New Record!\n",
            "Epoch   4/25 Batch   95/276 - Loss:  3.879, Seconds: 0.68\n",
            "Epoch   4/25 Batch  100/276 - Loss:  3.833, Seconds: 0.65\n",
            "Epoch   4/25 Batch  105/276 - Loss:  3.922, Seconds: 0.69\n",
            "Epoch   4/25 Batch  110/276 - Loss:  3.895, Seconds: 0.66\n",
            "Epoch   4/25 Batch  115/276 - Loss:  3.865, Seconds: 0.67\n",
            "Epoch   4/25 Batch  120/276 - Loss:  3.780, Seconds: 0.69\n",
            "Epoch   4/25 Batch  125/276 - Loss:  3.862, Seconds: 0.70\n",
            "Epoch   4/25 Batch  130/276 - Loss:  3.889, Seconds: 0.72\n",
            "Epoch   4/25 Batch  135/276 - Loss:  3.855, Seconds: 0.66\n",
            "Epoch   4/25 Batch  140/276 - Loss:  3.681, Seconds: 0.67\n",
            "Epoch   4/25 Batch  145/276 - Loss:  3.794, Seconds: 0.74\n",
            "Epoch   4/25 Batch  150/276 - Loss:  3.981, Seconds: 0.68\n",
            "Epoch   4/25 Batch  155/276 - Loss:  3.830, Seconds: 0.70\n",
            "Epoch   4/25 Batch  160/276 - Loss:  3.778, Seconds: 0.75\n",
            "Epoch   4/25 Batch  165/276 - Loss:  3.746, Seconds: 0.69\n",
            "Epoch   4/25 Batch  170/276 - Loss:  3.703, Seconds: 0.72\n",
            "Epoch   4/25 Batch  175/276 - Loss:  3.864, Seconds: 0.72\n",
            "Epoch   4/25 Batch  180/276 - Loss:  3.804, Seconds: 0.73\n",
            "Average loss for this update: 3.83\n",
            "New Record!\n",
            "Epoch   4/25 Batch  185/276 - Loss:  3.808, Seconds: 0.72\n",
            "Epoch   4/25 Batch  190/276 - Loss:  3.703, Seconds: 0.70\n",
            "Epoch   4/25 Batch  195/276 - Loss:  3.736, Seconds: 0.70\n",
            "Epoch   4/25 Batch  200/276 - Loss:  3.851, Seconds: 0.72\n",
            "Epoch   4/25 Batch  205/276 - Loss:  3.785, Seconds: 0.78\n",
            "Epoch   4/25 Batch  210/276 - Loss:  3.744, Seconds: 0.72\n",
            "Epoch   4/25 Batch  215/276 - Loss:  3.720, Seconds: 0.73\n",
            "Epoch   4/25 Batch  220/276 - Loss:  3.679, Seconds: 0.74\n",
            "Epoch   4/25 Batch  225/276 - Loss:  3.687, Seconds: 0.80\n",
            "Epoch   4/25 Batch  230/276 - Loss:  3.594, Seconds: 0.74\n",
            "Epoch   4/25 Batch  235/276 - Loss:  3.742, Seconds: 0.78\n",
            "Epoch   4/25 Batch  240/276 - Loss:  3.695, Seconds: 0.76\n",
            "Epoch   4/25 Batch  245/276 - Loss:  3.697, Seconds: 0.79\n",
            "Epoch   4/25 Batch  250/276 - Loss:  3.717, Seconds: 0.77\n",
            "Epoch   4/25 Batch  255/276 - Loss:  3.634, Seconds: 0.76\n",
            "Epoch   4/25 Batch  260/276 - Loss:  3.598, Seconds: 0.78\n",
            "Epoch   4/25 Batch  265/276 - Loss:  3.708, Seconds: 0.84\n",
            "Epoch   4/25 Batch  270/276 - Loss:  3.643, Seconds: 0.78\n",
            "Average loss for this update: 3.704\n",
            "New Record!\n",
            "Epoch   4/25 Batch  275/276 - Loss:  3.632, Seconds: 0.90\n",
            "Epoch   5/25 Batch    5/276 - Loss:  4.757, Seconds: 0.58\n",
            "Epoch   5/25 Batch   10/276 - Loss:  3.777, Seconds: 0.57\n",
            "Epoch   5/25 Batch   15/276 - Loss:  3.822, Seconds: 0.60\n",
            "Epoch   5/25 Batch   20/276 - Loss:  3.765, Seconds: 0.63\n",
            "Epoch   5/25 Batch   25/276 - Loss:  3.753, Seconds: 0.64\n",
            "Epoch   5/25 Batch   30/276 - Loss:  3.728, Seconds: 0.63\n",
            "Epoch   5/25 Batch   35/276 - Loss:  3.698, Seconds: 0.69\n",
            "Epoch   5/25 Batch   40/276 - Loss:  3.599, Seconds: 0.64\n",
            "Epoch   5/25 Batch   45/276 - Loss:  3.610, Seconds: 0.62\n",
            "Epoch   5/25 Batch   50/276 - Loss:  3.646, Seconds: 0.69\n",
            "Epoch   5/25 Batch   55/276 - Loss:  3.606, Seconds: 0.67\n",
            "Epoch   5/25 Batch   60/276 - Loss:  3.561, Seconds: 0.63\n",
            "Epoch   5/25 Batch   65/276 - Loss:  3.433, Seconds: 0.71\n",
            "Epoch   5/25 Batch   70/276 - Loss:  3.558, Seconds: 0.67\n",
            "Epoch   5/25 Batch   75/276 - Loss:  3.592, Seconds: 0.66\n",
            "Epoch   5/25 Batch   80/276 - Loss:  3.495, Seconds: 0.73\n",
            "Epoch   5/25 Batch   85/276 - Loss:  3.485, Seconds: 0.68\n",
            "Epoch   5/25 Batch   90/276 - Loss:  3.464, Seconds: 0.68\n",
            "Average loss for this update: 3.683\n",
            "New Record!\n",
            "Epoch   5/25 Batch   95/276 - Loss:  3.505, Seconds: 0.71\n",
            "Epoch   5/25 Batch  100/276 - Loss:  3.439, Seconds: 0.69\n",
            "Epoch   5/25 Batch  105/276 - Loss:  3.533, Seconds: 0.77\n",
            "Epoch   5/25 Batch  110/276 - Loss:  3.465, Seconds: 0.67\n",
            "Epoch   5/25 Batch  115/276 - Loss:  3.408, Seconds: 0.69\n",
            "Epoch   5/25 Batch  120/276 - Loss:  3.435, Seconds: 0.70\n",
            "Epoch   5/25 Batch  125/276 - Loss:  3.461, Seconds: 0.69\n",
            "Epoch   5/25 Batch  130/276 - Loss:  3.466, Seconds: 0.69\n",
            "Epoch   5/25 Batch  135/276 - Loss:  3.479, Seconds: 0.70\n",
            "Epoch   5/25 Batch  140/276 - Loss:  3.321, Seconds: 0.74\n",
            "Epoch   5/25 Batch  145/276 - Loss:  3.441, Seconds: 0.72\n",
            "Epoch   5/25 Batch  150/276 - Loss:  3.573, Seconds: 0.73\n",
            "Epoch   5/25 Batch  155/276 - Loss:  3.465, Seconds: 0.82\n",
            "Epoch   5/25 Batch  160/276 - Loss:  3.402, Seconds: 0.72\n",
            "Epoch   5/25 Batch  165/276 - Loss:  3.334, Seconds: 0.71\n",
            "Epoch   5/25 Batch  170/276 - Loss:  3.361, Seconds: 0.71\n",
            "Epoch   5/25 Batch  175/276 - Loss:  3.463, Seconds: 0.74\n",
            "Epoch   5/25 Batch  180/276 - Loss:  3.418, Seconds: 0.70\n",
            "Average loss for this update: 3.44\n",
            "New Record!\n",
            "Epoch   5/25 Batch  185/276 - Loss:  3.444, Seconds: 0.79\n",
            "Epoch   5/25 Batch  190/276 - Loss:  3.386, Seconds: 0.73\n",
            "Epoch   5/25 Batch  195/276 - Loss:  3.374, Seconds: 0.71\n",
            "Epoch   5/25 Batch  200/276 - Loss:  3.449, Seconds: 0.75\n",
            "Epoch   5/25 Batch  205/276 - Loss:  3.430, Seconds: 0.74\n",
            "Epoch   5/25 Batch  210/276 - Loss:  3.332, Seconds: 0.74\n",
            "Epoch   5/25 Batch  215/276 - Loss:  3.398, Seconds: 0.71\n",
            "Epoch   5/25 Batch  220/276 - Loss:  3.343, Seconds: 0.80\n",
            "Epoch   5/25 Batch  225/276 - Loss:  3.351, Seconds: 0.77\n",
            "Epoch   5/25 Batch  230/276 - Loss:  3.291, Seconds: 0.77\n",
            "Epoch   5/25 Batch  235/276 - Loss:  3.361, Seconds: 0.79\n",
            "Epoch   5/25 Batch  240/276 - Loss:  3.304, Seconds: 0.81\n",
            "Epoch   5/25 Batch  245/276 - Loss:  3.327, Seconds: 0.77\n",
            "Epoch   5/25 Batch  250/276 - Loss:  3.376, Seconds: 0.79\n",
            "Epoch   5/25 Batch  255/276 - Loss:  3.247, Seconds: 0.77\n",
            "Epoch   5/25 Batch  260/276 - Loss:  3.252, Seconds: 0.79\n",
            "Epoch   5/25 Batch  265/276 - Loss:  3.369, Seconds: 0.85\n",
            "Epoch   5/25 Batch  270/276 - Loss:  3.347, Seconds: 0.80\n",
            "Average loss for this update: 3.354\n",
            "New Record!\n",
            "Epoch   5/25 Batch  275/276 - Loss:  3.341, Seconds: 0.88\n",
            "Epoch   6/25 Batch    5/276 - Loss:  4.312, Seconds: 0.57\n",
            "Epoch   6/25 Batch   10/276 - Loss:  3.443, Seconds: 0.61\n",
            "Epoch   6/25 Batch   15/276 - Loss:  3.392, Seconds: 0.59\n",
            "Epoch   6/25 Batch   20/276 - Loss:  3.404, Seconds: 0.62\n",
            "Epoch   6/25 Batch   25/276 - Loss:  3.405, Seconds: 0.64\n",
            "Epoch   6/25 Batch   30/276 - Loss:  3.443, Seconds: 0.62\n",
            "Epoch   6/25 Batch   35/276 - Loss:  3.323, Seconds: 0.66\n",
            "Epoch   6/25 Batch   40/276 - Loss:  3.255, Seconds: 0.62\n",
            "Epoch   6/25 Batch   45/276 - Loss:  3.173, Seconds: 0.65\n",
            "Epoch   6/25 Batch   50/276 - Loss:  3.272, Seconds: 0.70\n",
            "Epoch   6/25 Batch   55/276 - Loss:  3.272, Seconds: 0.66\n",
            "Epoch   6/25 Batch   60/276 - Loss:  3.210, Seconds: 0.66\n",
            "Epoch   6/25 Batch   65/276 - Loss:  3.156, Seconds: 0.66\n",
            "Epoch   6/25 Batch   70/276 - Loss:  3.224, Seconds: 0.64\n",
            "Epoch   6/25 Batch   75/276 - Loss:  3.201, Seconds: 0.66\n",
            "Epoch   6/25 Batch   80/276 - Loss:  3.152, Seconds: 0.65\n",
            "Epoch   6/25 Batch   85/276 - Loss:  3.151, Seconds: 0.66\n",
            "Epoch   6/25 Batch   90/276 - Loss:  3.156, Seconds: 0.65\n",
            "Average loss for this update: 3.327\n",
            "New Record!\n",
            "Epoch   6/25 Batch   95/276 - Loss:  3.215, Seconds: 0.66\n",
            "Epoch   6/25 Batch  100/276 - Loss:  3.141, Seconds: 0.68\n",
            "Epoch   6/25 Batch  105/276 - Loss:  3.177, Seconds: 0.70\n",
            "Epoch   6/25 Batch  110/276 - Loss:  3.139, Seconds: 0.67\n",
            "Epoch   6/25 Batch  115/276 - Loss:  3.085, Seconds: 0.66\n",
            "Epoch   6/25 Batch  120/276 - Loss:  3.106, Seconds: 0.71\n",
            "Epoch   6/25 Batch  125/276 - Loss:  3.162, Seconds: 0.72\n",
            "Epoch   6/25 Batch  130/276 - Loss:  3.127, Seconds: 0.68\n",
            "Epoch   6/25 Batch  135/276 - Loss:  3.162, Seconds: 0.70\n",
            "Epoch   6/25 Batch  140/276 - Loss:  2.961, Seconds: 0.69\n",
            "Epoch   6/25 Batch  145/276 - Loss:  3.078, Seconds: 0.69\n",
            "Epoch   6/25 Batch  150/276 - Loss:  3.261, Seconds: 0.75\n",
            "Epoch   6/25 Batch  155/276 - Loss:  3.132, Seconds: 0.72\n",
            "Epoch   6/25 Batch  160/276 - Loss:  3.132, Seconds: 0.73\n",
            "Epoch   6/25 Batch  165/276 - Loss:  3.089, Seconds: 0.71\n",
            "Epoch   6/25 Batch  170/276 - Loss:  3.045, Seconds: 0.85\n",
            "Epoch   6/25 Batch  175/276 - Loss:  3.142, Seconds: 0.74\n",
            "Epoch   6/25 Batch  180/276 - Loss:  3.163, Seconds: 0.71\n",
            "Average loss for this update: 3.129\n",
            "New Record!\n",
            "Epoch   6/25 Batch  185/276 - Loss:  3.179, Seconds: 0.71\n",
            "Epoch   6/25 Batch  190/276 - Loss:  3.074, Seconds: 0.68\n",
            "Epoch   6/25 Batch  195/276 - Loss:  3.057, Seconds: 0.78\n",
            "Epoch   6/25 Batch  200/276 - Loss:  3.140, Seconds: 0.73\n",
            "Epoch   6/25 Batch  205/276 - Loss:  3.150, Seconds: 0.73\n",
            "Epoch   6/25 Batch  210/276 - Loss:  3.119, Seconds: 0.76\n",
            "Epoch   6/25 Batch  215/276 - Loss:  3.131, Seconds: 0.74\n",
            "Epoch   6/25 Batch  220/276 - Loss:  3.097, Seconds: 0.73\n",
            "Epoch   6/25 Batch  225/276 - Loss:  2.998, Seconds: 0.73\n",
            "Epoch   6/25 Batch  230/276 - Loss:  2.994, Seconds: 0.74\n",
            "Epoch   6/25 Batch  235/276 - Loss:  3.162, Seconds: 0.76\n",
            "Epoch   6/25 Batch  240/276 - Loss:  3.050, Seconds: 0.77\n",
            "Epoch   6/25 Batch  245/276 - Loss:  3.019, Seconds: 0.74\n",
            "Epoch   6/25 Batch  250/276 - Loss:  3.020, Seconds: 0.78\n",
            "Epoch   6/25 Batch  255/276 - Loss:  2.998, Seconds: 0.76\n",
            "Epoch   6/25 Batch  260/276 - Loss:  3.046, Seconds: 0.77\n",
            "Epoch   6/25 Batch  265/276 - Loss:  3.125, Seconds: 0.81\n",
            "Epoch   6/25 Batch  270/276 - Loss:  3.031, Seconds: 0.80\n",
            "Average loss for this update: 3.078\n",
            "New Record!\n",
            "Epoch   6/25 Batch  275/276 - Loss:  3.163, Seconds: 0.89\n",
            "Epoch   7/25 Batch    5/276 - Loss:  3.924, Seconds: 0.58\n",
            "Epoch   7/25 Batch   10/276 - Loss:  3.127, Seconds: 0.61\n",
            "Epoch   7/25 Batch   15/276 - Loss:  3.141, Seconds: 0.58\n",
            "Epoch   7/25 Batch   20/276 - Loss:  3.156, Seconds: 0.62\n",
            "Epoch   7/25 Batch   25/276 - Loss:  3.082, Seconds: 0.62\n",
            "Epoch   7/25 Batch   30/276 - Loss:  3.086, Seconds: 0.64\n",
            "Epoch   7/25 Batch   35/276 - Loss:  3.055, Seconds: 0.63\n",
            "Epoch   7/25 Batch   40/276 - Loss:  3.003, Seconds: 0.67\n",
            "Epoch   7/25 Batch   45/276 - Loss:  2.913, Seconds: 0.67\n",
            "Epoch   7/25 Batch   50/276 - Loss:  3.009, Seconds: 0.66\n",
            "Epoch   7/25 Batch   55/276 - Loss:  3.001, Seconds: 0.66\n",
            "Epoch   7/25 Batch   60/276 - Loss:  2.929, Seconds: 0.60\n",
            "Epoch   7/25 Batch   65/276 - Loss:  2.904, Seconds: 0.67\n",
            "Epoch   7/25 Batch   70/276 - Loss:  2.998, Seconds: 0.66\n",
            "Epoch   7/25 Batch   75/276 - Loss:  2.983, Seconds: 0.67\n",
            "Epoch   7/25 Batch   80/276 - Loss:  2.907, Seconds: 0.65\n",
            "Epoch   7/25 Batch   85/276 - Loss:  2.888, Seconds: 0.69\n",
            "Epoch   7/25 Batch   90/276 - Loss:  2.885, Seconds: 0.68\n",
            "Average loss for this update: 3.051\n",
            "New Record!\n",
            "Epoch   7/25 Batch   95/276 - Loss:  2.952, Seconds: 0.66\n",
            "Epoch   7/25 Batch  100/276 - Loss:  2.869, Seconds: 0.68\n",
            "Epoch   7/25 Batch  105/276 - Loss:  2.907, Seconds: 0.66\n",
            "Epoch   7/25 Batch  110/276 - Loss:  2.915, Seconds: 0.66\n",
            "Epoch   7/25 Batch  115/276 - Loss:  2.895, Seconds: 0.65\n",
            "Epoch   7/25 Batch  120/276 - Loss:  2.875, Seconds: 0.68\n",
            "Epoch   7/25 Batch  125/276 - Loss:  2.922, Seconds: 0.67\n",
            "Epoch   7/25 Batch  130/276 - Loss:  2.880, Seconds: 0.75\n",
            "Epoch   7/25 Batch  135/276 - Loss:  2.900, Seconds: 0.65\n",
            "Epoch   7/25 Batch  140/276 - Loss:  2.767, Seconds: 0.68\n",
            "Epoch   7/25 Batch  145/276 - Loss:  2.834, Seconds: 0.71\n",
            "Epoch   7/25 Batch  150/276 - Loss:  3.011, Seconds: 0.69\n",
            "Epoch   7/25 Batch  155/276 - Loss:  2.960, Seconds: 0.71\n",
            "Epoch   7/25 Batch  160/276 - Loss:  2.840, Seconds: 0.69\n",
            "Epoch   7/25 Batch  165/276 - Loss:  2.823, Seconds: 0.70\n",
            "Epoch   7/25 Batch  170/276 - Loss:  2.871, Seconds: 0.77\n",
            "Epoch   7/25 Batch  175/276 - Loss:  2.904, Seconds: 0.71\n",
            "Epoch   7/25 Batch  180/276 - Loss:  2.959, Seconds: 0.70\n",
            "Average loss for this update: 2.895\n",
            "New Record!\n",
            "Epoch   7/25 Batch  185/276 - Loss:  2.896, Seconds: 0.70\n",
            "Epoch   7/25 Batch  190/276 - Loss:  2.887, Seconds: 0.71\n",
            "Epoch   7/25 Batch  195/276 - Loss:  2.857, Seconds: 0.72\n",
            "Epoch   7/25 Batch  200/276 - Loss:  2.904, Seconds: 0.71\n",
            "Epoch   7/25 Batch  205/276 - Loss:  2.932, Seconds: 0.73\n",
            "Epoch   7/25 Batch  210/276 - Loss:  2.805, Seconds: 0.72\n",
            "Epoch   7/25 Batch  215/276 - Loss:  2.864, Seconds: 0.80\n",
            "Epoch   7/25 Batch  220/276 - Loss:  2.845, Seconds: 0.74\n",
            "Epoch   7/25 Batch  225/276 - Loss:  2.850, Seconds: 0.74\n",
            "Epoch   7/25 Batch  230/276 - Loss:  2.812, Seconds: 0.74\n",
            "Epoch   7/25 Batch  235/276 - Loss:  2.888, Seconds: 0.77\n",
            "Epoch   7/25 Batch  240/276 - Loss:  2.822, Seconds: 0.75\n",
            "Epoch   7/25 Batch  245/276 - Loss:  2.799, Seconds: 0.77\n",
            "Epoch   7/25 Batch  250/276 - Loss:  2.853, Seconds: 0.75\n",
            "Epoch   7/25 Batch  255/276 - Loss:  2.798, Seconds: 0.79\n",
            "Epoch   7/25 Batch  260/276 - Loss:  2.786, Seconds: 0.77\n",
            "Epoch   7/25 Batch  265/276 - Loss:  2.859, Seconds: 0.79\n",
            "Epoch   7/25 Batch  270/276 - Loss:  2.805, Seconds: 0.78\n",
            "Average loss for this update: 2.849\n",
            "New Record!\n",
            "Epoch   7/25 Batch  275/276 - Loss:  2.884, Seconds: 0.91\n",
            "Epoch   8/25 Batch    5/276 - Loss:  3.672, Seconds: 0.58\n",
            "Epoch   8/25 Batch   10/276 - Loss:  2.910, Seconds: 0.60\n",
            "Epoch   8/25 Batch   15/276 - Loss:  2.907, Seconds: 0.60\n",
            "Epoch   8/25 Batch   20/276 - Loss:  2.926, Seconds: 0.62\n",
            "Epoch   8/25 Batch   25/276 - Loss:  2.900, Seconds: 0.62\n",
            "Epoch   8/25 Batch   30/276 - Loss:  2.860, Seconds: 0.63\n",
            "Epoch   8/25 Batch   35/276 - Loss:  2.843, Seconds: 0.64\n",
            "Epoch   8/25 Batch   40/276 - Loss:  2.797, Seconds: 0.68\n",
            "Epoch   8/25 Batch   45/276 - Loss:  2.739, Seconds: 0.67\n",
            "Epoch   8/25 Batch   50/276 - Loss:  2.833, Seconds: 0.65\n",
            "Epoch   8/25 Batch   55/276 - Loss:  2.837, Seconds: 0.68\n",
            "Epoch   8/25 Batch   60/276 - Loss:  2.756, Seconds: 0.67\n",
            "Epoch   8/25 Batch   65/276 - Loss:  2.686, Seconds: 0.64\n",
            "Epoch   8/25 Batch   70/276 - Loss:  2.782, Seconds: 0.68\n",
            "Epoch   8/25 Batch   75/276 - Loss:  2.795, Seconds: 0.67\n",
            "Epoch   8/25 Batch   80/276 - Loss:  2.721, Seconds: 0.67\n",
            "Epoch   8/25 Batch   85/276 - Loss:  2.686, Seconds: 0.66\n",
            "Epoch   8/25 Batch   90/276 - Loss:  2.703, Seconds: 0.65\n",
            "Average loss for this update: 2.85\n",
            "No Improvement.\n",
            "Epoch   8/25 Batch   95/276 - Loss:  2.692, Seconds: 0.65\n",
            "Epoch   8/25 Batch  100/276 - Loss:  2.674, Seconds: 0.68\n",
            "Epoch   8/25 Batch  105/276 - Loss:  2.772, Seconds: 0.67\n",
            "Epoch   8/25 Batch  110/276 - Loss:  2.729, Seconds: 0.63\n",
            "Epoch   8/25 Batch  115/276 - Loss:  2.666, Seconds: 0.66\n",
            "Epoch   8/25 Batch  120/276 - Loss:  2.724, Seconds: 0.70\n",
            "Epoch   8/25 Batch  125/276 - Loss:  2.744, Seconds: 0.68\n",
            "Epoch   8/25 Batch  130/276 - Loss:  2.729, Seconds: 0.73\n",
            "Epoch   8/25 Batch  135/276 - Loss:  2.691, Seconds: 0.69\n",
            "Epoch   8/25 Batch  140/276 - Loss:  2.586, Seconds: 0.69\n",
            "Epoch   8/25 Batch  145/276 - Loss:  2.660, Seconds: 0.72\n",
            "Epoch   8/25 Batch  150/276 - Loss:  2.808, Seconds: 0.69\n",
            "Epoch   8/25 Batch  155/276 - Loss:  2.738, Seconds: 0.70\n",
            "Epoch   8/25 Batch  160/276 - Loss:  2.689, Seconds: 0.72\n",
            "Epoch   8/25 Batch  165/276 - Loss:  2.618, Seconds: 0.72\n",
            "Epoch   8/25 Batch  170/276 - Loss:  2.688, Seconds: 0.70\n",
            "Epoch   8/25 Batch  175/276 - Loss:  2.756, Seconds: 0.73\n",
            "Epoch   8/25 Batch  180/276 - Loss:  2.698, Seconds: 0.72\n",
            "Average loss for this update: 2.703\n",
            "New Record!\n",
            "Epoch   8/25 Batch  185/276 - Loss:  2.703, Seconds: 0.74\n",
            "Epoch   8/25 Batch  190/276 - Loss:  2.643, Seconds: 0.71\n",
            "Epoch   8/25 Batch  195/276 - Loss:  2.662, Seconds: 0.72\n",
            "Epoch   8/25 Batch  200/276 - Loss:  2.712, Seconds: 0.73\n",
            "Epoch   8/25 Batch  205/276 - Loss:  2.730, Seconds: 0.77\n",
            "Epoch   8/25 Batch  210/276 - Loss:  2.686, Seconds: 0.73\n",
            "Epoch   8/25 Batch  215/276 - Loss:  2.664, Seconds: 0.72\n",
            "Epoch   8/25 Batch  220/276 - Loss:  2.695, Seconds: 0.74\n",
            "Epoch   8/25 Batch  225/276 - Loss:  2.617, Seconds: 0.78\n",
            "Epoch   8/25 Batch  230/276 - Loss:  2.591, Seconds: 0.71\n",
            "Epoch   8/25 Batch  235/276 - Loss:  2.674, Seconds: 0.75\n",
            "Epoch   8/25 Batch  240/276 - Loss:  2.649, Seconds: 0.74\n",
            "Epoch   8/25 Batch  245/276 - Loss:  2.658, Seconds: 0.80\n",
            "Epoch   8/25 Batch  250/276 - Loss:  2.652, Seconds: 0.74\n",
            "Epoch   8/25 Batch  255/276 - Loss:  2.628, Seconds: 0.76\n",
            "Epoch   8/25 Batch  260/276 - Loss:  2.596, Seconds: 0.77\n",
            "Epoch   8/25 Batch  265/276 - Loss:  2.711, Seconds: 0.86\n",
            "Epoch   8/25 Batch  270/276 - Loss:  2.629, Seconds: 0.80\n",
            "Average loss for this update: 2.664\n",
            "New Record!\n",
            "Epoch   8/25 Batch  275/276 - Loss:  2.744, Seconds: 0.86\n",
            "Epoch   9/25 Batch    5/276 - Loss:  3.449, Seconds: 0.57\n",
            "Epoch   9/25 Batch   10/276 - Loss:  2.785, Seconds: 0.68\n",
            "Epoch   9/25 Batch   15/276 - Loss:  2.751, Seconds: 0.62\n",
            "Epoch   9/25 Batch   20/276 - Loss:  2.744, Seconds: 0.59\n",
            "Epoch   9/25 Batch   25/276 - Loss:  2.740, Seconds: 0.63\n",
            "Epoch   9/25 Batch   30/276 - Loss:  2.691, Seconds: 0.62\n",
            "Epoch   9/25 Batch   35/276 - Loss:  2.680, Seconds: 0.68\n",
            "Epoch   9/25 Batch   40/276 - Loss:  2.609, Seconds: 0.64\n",
            "Epoch   9/25 Batch   45/276 - Loss:  2.600, Seconds: 0.62\n",
            "Epoch   9/25 Batch   50/276 - Loss:  2.673, Seconds: 0.71\n",
            "Epoch   9/25 Batch   55/276 - Loss:  2.667, Seconds: 0.67\n",
            "Epoch   9/25 Batch   60/276 - Loss:  2.591, Seconds: 0.61\n",
            "Epoch   9/25 Batch   65/276 - Loss:  2.509, Seconds: 0.63\n",
            "Epoch   9/25 Batch   70/276 - Loss:  2.653, Seconds: 0.66\n",
            "Epoch   9/25 Batch   75/276 - Loss:  2.693, Seconds: 0.66\n",
            "Epoch   9/25 Batch   80/276 - Loss:  2.553, Seconds: 0.66\n",
            "Epoch   9/25 Batch   85/276 - Loss:  2.553, Seconds: 0.69\n",
            "Epoch   9/25 Batch   90/276 - Loss:  2.517, Seconds: 0.69\n",
            "Average loss for this update: 2.689\n",
            "No Improvement.\n",
            "Epoch   9/25 Batch   95/276 - Loss:  2.576, Seconds: 0.68\n",
            "Epoch   9/25 Batch  100/276 - Loss:  2.517, Seconds: 0.68\n",
            "Epoch   9/25 Batch  105/276 - Loss:  2.616, Seconds: 0.70\n",
            "Epoch   9/25 Batch  110/276 - Loss:  2.530, Seconds: 0.68\n",
            "Epoch   9/25 Batch  115/276 - Loss:  2.547, Seconds: 0.71\n",
            "Epoch   9/25 Batch  120/276 - Loss:  2.547, Seconds: 0.68\n",
            "Epoch   9/25 Batch  125/276 - Loss:  2.553, Seconds: 0.77\n",
            "Epoch   9/25 Batch  130/276 - Loss:  2.584, Seconds: 0.68\n",
            "Epoch   9/25 Batch  135/276 - Loss:  2.532, Seconds: 0.73\n",
            "Epoch   9/25 Batch  140/276 - Loss:  2.404, Seconds: 0.76\n",
            "Epoch   9/25 Batch  145/276 - Loss:  2.521, Seconds: 0.70\n",
            "Epoch   9/25 Batch  150/276 - Loss:  2.660, Seconds: 0.70\n",
            "Epoch   9/25 Batch  155/276 - Loss:  2.612, Seconds: 0.71\n",
            "Epoch   9/25 Batch  160/276 - Loss:  2.542, Seconds: 0.69\n",
            "Epoch   9/25 Batch  165/276 - Loss:  2.481, Seconds: 0.69\n",
            "Epoch   9/25 Batch  170/276 - Loss:  2.467, Seconds: 0.70\n",
            "Epoch   9/25 Batch  175/276 - Loss:  2.602, Seconds: 0.70\n",
            "Epoch   9/25 Batch  180/276 - Loss:  2.585, Seconds: 0.72\n",
            "Average loss for this update: 2.55\n",
            "New Record!\n",
            "Epoch   9/25 Batch  185/276 - Loss:  2.576, Seconds: 0.77\n",
            "Epoch   9/25 Batch  190/276 - Loss:  2.547, Seconds: 0.68\n",
            "Epoch   9/25 Batch  195/276 - Loss:  2.507, Seconds: 0.72\n",
            "Epoch   9/25 Batch  200/276 - Loss:  2.596, Seconds: 0.77\n",
            "Epoch   9/25 Batch  205/276 - Loss:  2.570, Seconds: 0.72\n",
            "Epoch   9/25 Batch  210/276 - Loss:  2.523, Seconds: 0.72\n",
            "Epoch   9/25 Batch  215/276 - Loss:  2.480, Seconds: 0.73\n",
            "Epoch   9/25 Batch  220/276 - Loss:  2.550, Seconds: 0.78\n",
            "Epoch   9/25 Batch  225/276 - Loss:  2.510, Seconds: 0.78\n",
            "Epoch   9/25 Batch  230/276 - Loss:  2.435, Seconds: 0.74\n",
            "Epoch   9/25 Batch  235/276 - Loss:  2.555, Seconds: 0.74\n",
            "Epoch   9/25 Batch  240/276 - Loss:  2.476, Seconds: 0.77\n",
            "Epoch   9/25 Batch  245/276 - Loss:  2.488, Seconds: 0.80\n",
            "Epoch   9/25 Batch  250/276 - Loss:  2.497, Seconds: 0.83\n",
            "Epoch   9/25 Batch  255/276 - Loss:  2.494, Seconds: 0.74\n",
            "Epoch   9/25 Batch  260/276 - Loss:  2.482, Seconds: 0.80\n",
            "Epoch   9/25 Batch  265/276 - Loss:  2.565, Seconds: 0.86\n",
            "Epoch   9/25 Batch  270/276 - Loss:  2.480, Seconds: 0.83\n",
            "Average loss for this update: 2.52\n",
            "New Record!\n",
            "Epoch   9/25 Batch  275/276 - Loss:  2.610, Seconds: 0.91\n",
            "Epoch  10/25 Batch    5/276 - Loss:  3.292, Seconds: 0.59\n",
            "Epoch  10/25 Batch   10/276 - Loss:  2.670, Seconds: 0.60\n",
            "Epoch  10/25 Batch   15/276 - Loss:  2.625, Seconds: 0.63\n",
            "Epoch  10/25 Batch   20/276 - Loss:  2.651, Seconds: 0.69\n",
            "Epoch  10/25 Batch   25/276 - Loss:  2.598, Seconds: 0.69\n",
            "Epoch  10/25 Batch   30/276 - Loss:  2.549, Seconds: 0.63\n",
            "Epoch  10/25 Batch   35/276 - Loss:  2.530, Seconds: 0.61\n",
            "Epoch  10/25 Batch   40/276 - Loss:  2.499, Seconds: 0.65\n",
            "Epoch  10/25 Batch   45/276 - Loss:  2.428, Seconds: 0.64\n",
            "Epoch  10/25 Batch   50/276 - Loss:  2.509, Seconds: 0.67\n",
            "Epoch  10/25 Batch   55/276 - Loss:  2.511, Seconds: 0.65\n",
            "Epoch  10/25 Batch   60/276 - Loss:  2.461, Seconds: 0.65\n",
            "Epoch  10/25 Batch   65/276 - Loss:  2.392, Seconds: 0.64\n",
            "Epoch  10/25 Batch   70/276 - Loss:  2.491, Seconds: 0.65\n",
            "Epoch  10/25 Batch   75/276 - Loss:  2.540, Seconds: 0.65\n",
            "Epoch  10/25 Batch   80/276 - Loss:  2.429, Seconds: 0.71\n",
            "Epoch  10/25 Batch   85/276 - Loss:  2.407, Seconds: 0.66\n",
            "Epoch  10/25 Batch   90/276 - Loss:  2.387, Seconds: 0.65\n",
            "Average loss for this update: 2.551\n",
            "No Improvement.\n",
            "Epoch  10/25 Batch   95/276 - Loss:  2.403, Seconds: 0.73\n",
            "Epoch  10/25 Batch  100/276 - Loss:  2.394, Seconds: 0.66\n",
            "Epoch  10/25 Batch  105/276 - Loss:  2.430, Seconds: 0.65\n",
            "Epoch  10/25 Batch  110/276 - Loss:  2.421, Seconds: 0.68\n",
            "Epoch  10/25 Batch  115/276 - Loss:  2.384, Seconds: 0.68\n",
            "Epoch  10/25 Batch  120/276 - Loss:  2.411, Seconds: 0.69\n",
            "Epoch  10/25 Batch  125/276 - Loss:  2.419, Seconds: 0.72\n",
            "Epoch  10/25 Batch  130/276 - Loss:  2.441, Seconds: 0.68\n",
            "Epoch  10/25 Batch  135/276 - Loss:  2.395, Seconds: 0.71\n",
            "Epoch  10/25 Batch  140/276 - Loss:  2.332, Seconds: 0.68\n",
            "Epoch  10/25 Batch  145/276 - Loss:  2.358, Seconds: 0.73\n",
            "Epoch  10/25 Batch  150/276 - Loss:  2.531, Seconds: 0.69\n",
            "Epoch  10/25 Batch  155/276 - Loss:  2.489, Seconds: 0.69\n",
            "Epoch  10/25 Batch  160/276 - Loss:  2.428, Seconds: 0.69\n",
            "Epoch  10/25 Batch  165/276 - Loss:  2.351, Seconds: 0.70\n",
            "Epoch  10/25 Batch  170/276 - Loss:  2.376, Seconds: 0.76\n",
            "Epoch  10/25 Batch  175/276 - Loss:  2.441, Seconds: 0.75\n",
            "Epoch  10/25 Batch  180/276 - Loss:  2.424, Seconds: 0.71\n",
            "Average loss for this update: 2.413\n",
            "New Record!\n",
            "Epoch  10/25 Batch  185/276 - Loss:  2.405, Seconds: 0.72\n",
            "Epoch  10/25 Batch  190/276 - Loss:  2.409, Seconds: 0.74\n",
            "Epoch  10/25 Batch  195/276 - Loss:  2.371, Seconds: 0.71\n",
            "Epoch  10/25 Batch  200/276 - Loss:  2.474, Seconds: 0.73\n",
            "Epoch  10/25 Batch  205/276 - Loss:  2.462, Seconds: 0.73\n",
            "Epoch  10/25 Batch  210/276 - Loss:  2.449, Seconds: 0.72\n",
            "Epoch  10/25 Batch  215/276 - Loss:  2.398, Seconds: 0.73\n",
            "Epoch  10/25 Batch  220/276 - Loss:  2.396, Seconds: 0.75\n",
            "Epoch  10/25 Batch  225/276 - Loss:  2.389, Seconds: 0.78\n",
            "Epoch  10/25 Batch  230/276 - Loss:  2.293, Seconds: 0.72\n",
            "Epoch  10/25 Batch  235/276 - Loss:  2.392, Seconds: 0.75\n",
            "Epoch  10/25 Batch  240/276 - Loss:  2.336, Seconds: 0.75\n",
            "Epoch  10/25 Batch  245/276 - Loss:  2.335, Seconds: 0.79\n",
            "Epoch  10/25 Batch  250/276 - Loss:  2.377, Seconds: 0.75\n",
            "Epoch  10/25 Batch  255/276 - Loss:  2.399, Seconds: 0.75\n",
            "Epoch  10/25 Batch  260/276 - Loss:  2.330, Seconds: 0.77\n",
            "Epoch  10/25 Batch  265/276 - Loss:  2.477, Seconds: 0.78\n",
            "Epoch  10/25 Batch  270/276 - Loss:  2.398, Seconds: 0.82\n",
            "Average loss for this update: 2.398\n",
            "New Record!\n",
            "Epoch  10/25 Batch  275/276 - Loss:  2.491, Seconds: 0.88\n",
            "Epoch  11/25 Batch    5/276 - Loss:  3.213, Seconds: 0.59\n",
            "Epoch  11/25 Batch   10/276 - Loss:  2.532, Seconds: 0.59\n",
            "Epoch  11/25 Batch   15/276 - Loss:  2.502, Seconds: 0.64\n",
            "Epoch  11/25 Batch   20/276 - Loss:  2.522, Seconds: 0.60\n",
            "Epoch  11/25 Batch   25/276 - Loss:  2.504, Seconds: 0.65\n",
            "Epoch  11/25 Batch   30/276 - Loss:  2.448, Seconds: 0.66\n",
            "Epoch  11/25 Batch   35/276 - Loss:  2.459, Seconds: 0.68\n",
            "Epoch  11/25 Batch   40/276 - Loss:  2.378, Seconds: 0.69\n",
            "Epoch  11/25 Batch   45/276 - Loss:  2.327, Seconds: 0.63\n",
            "Epoch  11/25 Batch   50/276 - Loss:  2.431, Seconds: 0.65\n",
            "Epoch  11/25 Batch   55/276 - Loss:  2.407, Seconds: 0.63\n",
            "Epoch  11/25 Batch   60/276 - Loss:  2.307, Seconds: 0.64\n",
            "Epoch  11/25 Batch   65/276 - Loss:  2.291, Seconds: 0.63\n",
            "Epoch  11/25 Batch   70/276 - Loss:  2.412, Seconds: 0.66\n",
            "Epoch  11/25 Batch   75/276 - Loss:  2.416, Seconds: 0.68\n",
            "Epoch  11/25 Batch   80/276 - Loss:  2.335, Seconds: 0.66\n",
            "Epoch  11/25 Batch   85/276 - Loss:  2.312, Seconds: 0.65\n",
            "Epoch  11/25 Batch   90/276 - Loss:  2.299, Seconds: 0.66\n",
            "Average loss for this update: 2.446\n",
            "No Improvement.\n",
            "Epoch  11/25 Batch   95/276 - Loss:  2.335, Seconds: 0.69\n",
            "Epoch  11/25 Batch  100/276 - Loss:  2.283, Seconds: 0.68\n",
            "Epoch  11/25 Batch  105/276 - Loss:  2.332, Seconds: 0.72\n",
            "Epoch  11/25 Batch  110/276 - Loss:  2.298, Seconds: 0.67\n",
            "Epoch  11/25 Batch  115/276 - Loss:  2.289, Seconds: 0.74\n",
            "Epoch  11/25 Batch  120/276 - Loss:  2.307, Seconds: 0.66\n",
            "Epoch  11/25 Batch  125/276 - Loss:  2.273, Seconds: 0.69\n",
            "Epoch  11/25 Batch  130/276 - Loss:  2.358, Seconds: 0.70\n",
            "Epoch  11/25 Batch  135/276 - Loss:  2.290, Seconds: 0.68\n",
            "Epoch  11/25 Batch  140/276 - Loss:  2.221, Seconds: 0.69\n",
            "Epoch  11/25 Batch  145/276 - Loss:  2.241, Seconds: 0.75\n",
            "Epoch  11/25 Batch  150/276 - Loss:  2.449, Seconds: 0.68\n",
            "Epoch  11/25 Batch  155/276 - Loss:  2.297, Seconds: 0.70\n",
            "Epoch  11/25 Batch  160/276 - Loss:  2.319, Seconds: 0.72\n",
            "Epoch  11/25 Batch  165/276 - Loss:  2.265, Seconds: 0.68\n",
            "Epoch  11/25 Batch  170/276 - Loss:  2.281, Seconds: 0.70\n",
            "Epoch  11/25 Batch  175/276 - Loss:  2.326, Seconds: 0.71\n",
            "Epoch  11/25 Batch  180/276 - Loss:  2.320, Seconds: 0.70\n",
            "Average loss for this update: 2.303\n",
            "New Record!\n",
            "Epoch  11/25 Batch  185/276 - Loss:  2.265, Seconds: 0.74\n",
            "Epoch  11/25 Batch  190/276 - Loss:  2.282, Seconds: 0.72\n",
            "Epoch  11/25 Batch  195/276 - Loss:  2.260, Seconds: 0.73\n",
            "Epoch  11/25 Batch  200/276 - Loss:  2.312, Seconds: 0.71\n",
            "Epoch  11/25 Batch  205/276 - Loss:  2.374, Seconds: 0.74\n",
            "Epoch  11/25 Batch  210/276 - Loss:  2.307, Seconds: 0.73\n",
            "Epoch  11/25 Batch  215/276 - Loss:  2.264, Seconds: 0.73\n",
            "Epoch  11/25 Batch  220/276 - Loss:  2.337, Seconds: 0.75\n",
            "Epoch  11/25 Batch  225/276 - Loss:  2.252, Seconds: 0.73\n",
            "Epoch  11/25 Batch  230/276 - Loss:  2.180, Seconds: 0.74\n",
            "Epoch  11/25 Batch  235/276 - Loss:  2.330, Seconds: 0.75\n",
            "Epoch  11/25 Batch  240/276 - Loss:  2.252, Seconds: 0.75\n",
            "Epoch  11/25 Batch  245/276 - Loss:  2.288, Seconds: 0.75\n",
            "Epoch  11/25 Batch  250/276 - Loss:  2.262, Seconds: 0.79\n",
            "Epoch  11/25 Batch  255/276 - Loss:  2.335, Seconds: 0.77\n",
            "Epoch  11/25 Batch  260/276 - Loss:  2.264, Seconds: 0.77\n",
            "Epoch  11/25 Batch  265/276 - Loss:  2.346, Seconds: 0.79\n",
            "Epoch  11/25 Batch  270/276 - Loss:  2.351, Seconds: 0.83\n",
            "Average loss for this update: 2.298\n",
            "New Record!\n",
            "Epoch  11/25 Batch  275/276 - Loss:  2.416, Seconds: 0.91\n",
            "Epoch  12/25 Batch    5/276 - Loss:  3.092, Seconds: 0.62\n",
            "Epoch  12/25 Batch   10/276 - Loss:  2.445, Seconds: 0.58\n",
            "Epoch  12/25 Batch   15/276 - Loss:  2.399, Seconds: 0.59\n",
            "Epoch  12/25 Batch   20/276 - Loss:  2.470, Seconds: 0.60\n",
            "Epoch  12/25 Batch   25/276 - Loss:  2.457, Seconds: 0.64\n",
            "Epoch  12/25 Batch   30/276 - Loss:  2.394, Seconds: 0.63\n",
            "Epoch  12/25 Batch   35/276 - Loss:  2.395, Seconds: 0.71\n",
            "Epoch  12/25 Batch   40/276 - Loss:  2.310, Seconds: 0.64\n",
            "Epoch  12/25 Batch   45/276 - Loss:  2.256, Seconds: 0.62\n",
            "Epoch  12/25 Batch   50/276 - Loss:  2.334, Seconds: 0.63\n",
            "Epoch  12/25 Batch   55/276 - Loss:  2.333, Seconds: 0.64\n",
            "Epoch  12/25 Batch   60/276 - Loss:  2.201, Seconds: 0.68\n",
            "Epoch  12/25 Batch   65/276 - Loss:  2.183, Seconds: 0.64\n",
            "Epoch  12/25 Batch   70/276 - Loss:  2.306, Seconds: 0.65\n",
            "Epoch  12/25 Batch   75/276 - Loss:  2.373, Seconds: 0.70\n",
            "Epoch  12/25 Batch   80/276 - Loss:  2.256, Seconds: 0.67\n",
            "Epoch  12/25 Batch   85/276 - Loss:  2.261, Seconds: 0.65\n",
            "Epoch  12/25 Batch   90/276 - Loss:  2.188, Seconds: 0.65\n",
            "Average loss for this update: 2.366\n",
            "No Improvement.\n",
            "Epoch  12/25 Batch   95/276 - Loss:  2.247, Seconds: 0.66\n",
            "Epoch  12/25 Batch  100/276 - Loss:  2.206, Seconds: 0.67\n",
            "Epoch  12/25 Batch  105/276 - Loss:  2.233, Seconds: 0.68\n",
            "Epoch  12/25 Batch  110/276 - Loss:  2.244, Seconds: 0.66\n",
            "Epoch  12/25 Batch  115/276 - Loss:  2.204, Seconds: 0.67\n",
            "Epoch  12/25 Batch  120/276 - Loss:  2.251, Seconds: 0.70\n",
            "Epoch  12/25 Batch  125/276 - Loss:  2.225, Seconds: 0.66\n",
            "Epoch  12/25 Batch  130/276 - Loss:  2.216, Seconds: 0.71\n",
            "Epoch  12/25 Batch  135/276 - Loss:  2.202, Seconds: 0.72\n",
            "Epoch  12/25 Batch  140/276 - Loss:  2.106, Seconds: 0.67\n",
            "Epoch  12/25 Batch  145/276 - Loss:  2.190, Seconds: 0.71\n",
            "Epoch  12/25 Batch  150/276 - Loss:  2.298, Seconds: 0.73\n",
            "Epoch  12/25 Batch  155/276 - Loss:  2.297, Seconds: 0.68\n",
            "Epoch  12/25 Batch  160/276 - Loss:  2.238, Seconds: 0.70\n",
            "Epoch  12/25 Batch  165/276 - Loss:  2.146, Seconds: 0.70\n",
            "Epoch  12/25 Batch  170/276 - Loss:  2.156, Seconds: 0.77\n",
            "Epoch  12/25 Batch  175/276 - Loss:  2.220, Seconds: 0.70\n",
            "Epoch  12/25 Batch  180/276 - Loss:  2.220, Seconds: 0.79\n",
            "Average loss for this update: 2.218\n",
            "New Record!\n",
            "Epoch  12/25 Batch  185/276 - Loss:  2.229, Seconds: 0.74\n",
            "Epoch  12/25 Batch  190/276 - Loss:  2.217, Seconds: 0.68\n",
            "Epoch  12/25 Batch  195/276 - Loss:  2.191, Seconds: 0.72\n",
            "Epoch  12/25 Batch  200/276 - Loss:  2.258, Seconds: 0.73\n",
            "Epoch  12/25 Batch  205/276 - Loss:  2.294, Seconds: 0.71\n",
            "Epoch  12/25 Batch  210/276 - Loss:  2.185, Seconds: 0.76\n",
            "Epoch  12/25 Batch  215/276 - Loss:  2.198, Seconds: 0.77\n",
            "Epoch  12/25 Batch  220/276 - Loss:  2.208, Seconds: 0.76\n",
            "Epoch  12/25 Batch  225/276 - Loss:  2.161, Seconds: 0.75\n",
            "Epoch  12/25 Batch  230/276 - Loss:  2.116, Seconds: 0.79\n",
            "Epoch  12/25 Batch  235/276 - Loss:  2.227, Seconds: 0.79\n",
            "Epoch  12/25 Batch  240/276 - Loss:  2.207, Seconds: 0.74\n",
            "Epoch  12/25 Batch  245/276 - Loss:  2.227, Seconds: 0.75\n",
            "Epoch  12/25 Batch  250/276 - Loss:  2.213, Seconds: 0.75\n",
            "Epoch  12/25 Batch  255/276 - Loss:  2.254, Seconds: 0.80\n",
            "Epoch  12/25 Batch  260/276 - Loss:  2.144, Seconds: 0.76\n",
            "Epoch  12/25 Batch  265/276 - Loss:  2.291, Seconds: 0.86\n",
            "Epoch  12/25 Batch  270/276 - Loss:  2.236, Seconds: 0.82\n",
            "Average loss for this update: 2.217\n",
            "New Record!\n",
            "Epoch  12/25 Batch  275/276 - Loss:  2.317, Seconds: 0.91\n",
            "Epoch  13/25 Batch    5/276 - Loss:  3.004, Seconds: 0.60\n",
            "Epoch  13/25 Batch   10/276 - Loss:  2.416, Seconds: 0.58\n",
            "Epoch  13/25 Batch   15/276 - Loss:  2.353, Seconds: 0.61\n",
            "Epoch  13/25 Batch   20/276 - Loss:  2.373, Seconds: 0.61\n",
            "Epoch  13/25 Batch   25/276 - Loss:  2.380, Seconds: 0.61\n",
            "Epoch  13/25 Batch   30/276 - Loss:  2.282, Seconds: 0.62\n",
            "Epoch  13/25 Batch   35/276 - Loss:  2.343, Seconds: 0.69\n",
            "Epoch  13/25 Batch   40/276 - Loss:  2.248, Seconds: 0.63\n",
            "Epoch  13/25 Batch   45/276 - Loss:  2.169, Seconds: 0.66\n",
            "Epoch  13/25 Batch   50/276 - Loss:  2.288, Seconds: 0.70\n",
            "Epoch  13/25 Batch   55/276 - Loss:  2.216, Seconds: 0.63\n",
            "Epoch  13/25 Batch   60/276 - Loss:  2.162, Seconds: 0.63\n",
            "Epoch  13/25 Batch   65/276 - Loss:  2.105, Seconds: 0.66\n",
            "Epoch  13/25 Batch   70/276 - Loss:  2.212, Seconds: 0.66\n",
            "Epoch  13/25 Batch   75/276 - Loss:  2.294, Seconds: 0.64\n",
            "Epoch  13/25 Batch   80/276 - Loss:  2.228, Seconds: 0.66\n",
            "Epoch  13/25 Batch   85/276 - Loss:  2.173, Seconds: 0.65\n",
            "Epoch  13/25 Batch   90/276 - Loss:  2.185, Seconds: 0.66\n",
            "Average loss for this update: 2.299\n",
            "No Improvement.\n",
            "Epoch  13/25 Batch   95/276 - Loss:  2.153, Seconds: 0.66\n",
            "Epoch  13/25 Batch  100/276 - Loss:  2.187, Seconds: 0.68\n",
            "Epoch  13/25 Batch  105/276 - Loss:  2.179, Seconds: 0.66\n",
            "Epoch  13/25 Batch  110/276 - Loss:  2.119, Seconds: 0.65\n",
            "Epoch  13/25 Batch  115/276 - Loss:  2.188, Seconds: 0.68\n",
            "Epoch  13/25 Batch  120/276 - Loss:  2.160, Seconds: 0.70\n",
            "Epoch  13/25 Batch  125/276 - Loss:  2.201, Seconds: 0.70\n",
            "Epoch  13/25 Batch  130/276 - Loss:  2.154, Seconds: 0.69\n",
            "Epoch  13/25 Batch  135/276 - Loss:  2.127, Seconds: 0.67\n",
            "Epoch  13/25 Batch  140/276 - Loss:  2.023, Seconds: 0.74\n",
            "Epoch  13/25 Batch  145/276 - Loss:  2.103, Seconds: 0.69\n",
            "Epoch  13/25 Batch  150/276 - Loss:  2.268, Seconds: 0.76\n",
            "Epoch  13/25 Batch  155/276 - Loss:  2.178, Seconds: 0.71\n",
            "Epoch  13/25 Batch  160/276 - Loss:  2.113, Seconds: 0.69\n",
            "Epoch  13/25 Batch  165/276 - Loss:  2.080, Seconds: 0.69\n",
            "Epoch  13/25 Batch  170/276 - Loss:  2.077, Seconds: 0.72\n",
            "Epoch  13/25 Batch  175/276 - Loss:  2.135, Seconds: 0.71\n",
            "Epoch  13/25 Batch  180/276 - Loss:  2.175, Seconds: 0.70\n",
            "Average loss for this update: 2.144\n",
            "New Record!\n",
            "Epoch  13/25 Batch  185/276 - Loss:  2.159, Seconds: 0.74\n",
            "Epoch  13/25 Batch  190/276 - Loss:  2.130, Seconds: 0.71\n",
            "Epoch  13/25 Batch  195/276 - Loss:  2.103, Seconds: 0.73\n",
            "Epoch  13/25 Batch  200/276 - Loss:  2.179, Seconds: 0.74\n",
            "Epoch  13/25 Batch  205/276 - Loss:  2.192, Seconds: 0.76\n",
            "Epoch  13/25 Batch  210/276 - Loss:  2.136, Seconds: 0.71\n",
            "Epoch  13/25 Batch  215/276 - Loss:  2.145, Seconds: 0.71\n",
            "Epoch  13/25 Batch  220/276 - Loss:  2.145, Seconds: 0.71\n",
            "Epoch  13/25 Batch  225/276 - Loss:  2.124, Seconds: 0.78\n",
            "Epoch  13/25 Batch  230/276 - Loss:  2.069, Seconds: 0.72\n",
            "Epoch  13/25 Batch  235/276 - Loss:  2.160, Seconds: 0.74\n",
            "Epoch  13/25 Batch  240/276 - Loss:  2.108, Seconds: 0.85\n",
            "Epoch  13/25 Batch  245/276 - Loss:  2.065, Seconds: 0.79\n",
            "Epoch  13/25 Batch  250/276 - Loss:  2.120, Seconds: 0.76\n",
            "Epoch  13/25 Batch  255/276 - Loss:  2.184, Seconds: 0.78\n",
            "Epoch  13/25 Batch  260/276 - Loss:  2.103, Seconds: 0.77\n",
            "Epoch  13/25 Batch  265/276 - Loss:  2.180, Seconds: 0.85\n",
            "Epoch  13/25 Batch  270/276 - Loss:  2.126, Seconds: 0.76\n",
            "Average loss for this update: 2.141\n",
            "New Record!\n",
            "Epoch  13/25 Batch  275/276 - Loss:  2.283, Seconds: 0.90\n",
            "Epoch  14/25 Batch    5/276 - Loss:  2.976, Seconds: 0.59\n",
            "Epoch  14/25 Batch   10/276 - Loss:  2.356, Seconds: 0.59\n",
            "Epoch  14/25 Batch   15/276 - Loss:  2.329, Seconds: 0.66\n",
            "Epoch  14/25 Batch   20/276 - Loss:  2.329, Seconds: 0.61\n",
            "Epoch  14/25 Batch   25/276 - Loss:  2.330, Seconds: 0.65\n",
            "Epoch  14/25 Batch   30/276 - Loss:  2.275, Seconds: 0.65\n",
            "Epoch  14/25 Batch   35/276 - Loss:  2.270, Seconds: 0.61\n",
            "Epoch  14/25 Batch   40/276 - Loss:  2.210, Seconds: 0.63\n",
            "Epoch  14/25 Batch   45/276 - Loss:  2.124, Seconds: 0.64\n",
            "Epoch  14/25 Batch   50/276 - Loss:  2.185, Seconds: 0.64\n",
            "Epoch  14/25 Batch   55/276 - Loss:  2.197, Seconds: 0.64\n",
            "Epoch  14/25 Batch   60/276 - Loss:  2.097, Seconds: 0.62\n",
            "Epoch  14/25 Batch   65/276 - Loss:  2.045, Seconds: 0.66\n",
            "Epoch  14/25 Batch   70/276 - Loss:  2.138, Seconds: 0.70\n",
            "Epoch  14/25 Batch   75/276 - Loss:  2.160, Seconds: 0.65\n",
            "Epoch  14/25 Batch   80/276 - Loss:  2.125, Seconds: 0.66\n",
            "Epoch  14/25 Batch   85/276 - Loss:  2.075, Seconds: 0.69\n",
            "Epoch  14/25 Batch   90/276 - Loss:  2.072, Seconds: 0.65\n",
            "Average loss for this update: 2.236\n",
            "No Improvement.\n",
            "Epoch  14/25 Batch   95/276 - Loss:  2.129, Seconds: 0.68\n",
            "Epoch  14/25 Batch  100/276 - Loss:  2.065, Seconds: 0.72\n",
            "Epoch  14/25 Batch  105/276 - Loss:  2.072, Seconds: 0.67\n",
            "Epoch  14/25 Batch  110/276 - Loss:  2.039, Seconds: 0.65\n",
            "Epoch  14/25 Batch  115/276 - Loss:  2.065, Seconds: 0.73\n",
            "Epoch  14/25 Batch  120/276 - Loss:  2.098, Seconds: 0.70\n",
            "Epoch  14/25 Batch  125/276 - Loss:  2.111, Seconds: 0.69\n",
            "Epoch  14/25 Batch  130/276 - Loss:  2.122, Seconds: 0.68\n",
            "Epoch  14/25 Batch  135/276 - Loss:  2.088, Seconds: 0.69\n",
            "Epoch  14/25 Batch  140/276 - Loss:  1.960, Seconds: 0.68\n",
            "Epoch  14/25 Batch  145/276 - Loss:  2.035, Seconds: 0.70\n",
            "Epoch  14/25 Batch  150/276 - Loss:  2.179, Seconds: 0.70\n",
            "Epoch  14/25 Batch  155/276 - Loss:  2.124, Seconds: 0.69\n",
            "Epoch  14/25 Batch  160/276 - Loss:  2.110, Seconds: 0.69\n",
            "Epoch  14/25 Batch  165/276 - Loss:  1.993, Seconds: 0.76\n",
            "Epoch  14/25 Batch  170/276 - Loss:  2.045, Seconds: 0.68\n",
            "Epoch  14/25 Batch  175/276 - Loss:  2.049, Seconds: 0.78\n",
            "Epoch  14/25 Batch  180/276 - Loss:  2.055, Seconds: 0.78\n",
            "Average loss for this update: 2.075\n",
            "New Record!\n",
            "Epoch  14/25 Batch  185/276 - Loss:  2.070, Seconds: 0.75\n",
            "Epoch  14/25 Batch  190/276 - Loss:  2.047, Seconds: 0.70\n",
            "Epoch  14/25 Batch  195/276 - Loss:  1.992, Seconds: 0.76\n",
            "Epoch  14/25 Batch  200/276 - Loss:  2.117, Seconds: 0.73\n",
            "Epoch  14/25 Batch  205/276 - Loss:  2.175, Seconds: 0.70\n",
            "Epoch  14/25 Batch  210/276 - Loss:  2.052, Seconds: 0.76\n",
            "Epoch  14/25 Batch  215/276 - Loss:  2.074, Seconds: 0.72\n",
            "Epoch  14/25 Batch  220/276 - Loss:  2.090, Seconds: 0.73\n",
            "Epoch  14/25 Batch  225/276 - Loss:  2.041, Seconds: 0.72\n",
            "Epoch  14/25 Batch  230/276 - Loss:  2.016, Seconds: 0.74\n",
            "Epoch  14/25 Batch  235/276 - Loss:  2.066, Seconds: 0.79\n",
            "Epoch  14/25 Batch  240/276 - Loss:  2.018, Seconds: 0.74\n",
            "Epoch  14/25 Batch  245/276 - Loss:  2.038, Seconds: 0.76\n",
            "Epoch  14/25 Batch  250/276 - Loss:  2.054, Seconds: 0.81\n",
            "Epoch  14/25 Batch  255/276 - Loss:  2.167, Seconds: 0.81\n",
            "Epoch  14/25 Batch  260/276 - Loss:  2.094, Seconds: 0.78\n",
            "Epoch  14/25 Batch  265/276 - Loss:  2.125, Seconds: 0.79\n",
            "Epoch  14/25 Batch  270/276 - Loss:  2.054, Seconds: 0.80\n",
            "Average loss for this update: 2.076\n",
            "No Improvement.\n",
            "Epoch  14/25 Batch  275/276 - Loss:  2.206, Seconds: 0.95\n",
            "Epoch  15/25 Batch    5/276 - Loss:  2.912, Seconds: 0.59\n",
            "Epoch  15/25 Batch   10/276 - Loss:  2.310, Seconds: 0.61\n",
            "Epoch  15/25 Batch   15/276 - Loss:  2.302, Seconds: 0.65\n",
            "Epoch  15/25 Batch   20/276 - Loss:  2.293, Seconds: 0.62\n",
            "Epoch  15/25 Batch   25/276 - Loss:  2.255, Seconds: 0.64\n",
            "Epoch  15/25 Batch   30/276 - Loss:  2.247, Seconds: 0.64\n",
            "Epoch  15/25 Batch   35/276 - Loss:  2.152, Seconds: 0.62\n",
            "Epoch  15/25 Batch   40/276 - Loss:  2.132, Seconds: 0.66\n",
            "Epoch  15/25 Batch   45/276 - Loss:  2.072, Seconds: 0.63\n",
            "Epoch  15/25 Batch   50/276 - Loss:  2.105, Seconds: 0.63\n",
            "Epoch  15/25 Batch   55/276 - Loss:  2.147, Seconds: 0.66\n",
            "Epoch  15/25 Batch   60/276 - Loss:  2.011, Seconds: 0.62\n",
            "Epoch  15/25 Batch   65/276 - Loss:  1.985, Seconds: 0.66\n",
            "Epoch  15/25 Batch   70/276 - Loss:  2.092, Seconds: 0.64\n",
            "Epoch  15/25 Batch   75/276 - Loss:  2.140, Seconds: 0.66\n",
            "Epoch  15/25 Batch   80/276 - Loss:  2.059, Seconds: 0.67\n",
            "Epoch  15/25 Batch   85/276 - Loss:  2.030, Seconds: 0.66\n",
            "Epoch  15/25 Batch   90/276 - Loss:  2.070, Seconds: 0.65\n",
            "Average loss for this update: 2.181\n",
            "No Improvement.\n",
            "Epoch  15/25 Batch   95/276 - Loss:  2.059, Seconds: 0.65\n",
            "Epoch  15/25 Batch  100/276 - Loss:  1.995, Seconds: 0.67\n",
            "Epoch  15/25 Batch  105/276 - Loss:  1.991, Seconds: 0.67\n",
            "Epoch  15/25 Batch  110/276 - Loss:  1.976, Seconds: 0.67\n",
            "Epoch  15/25 Batch  115/276 - Loss:  1.945, Seconds: 0.68\n",
            "Epoch  15/25 Batch  120/276 - Loss:  2.038, Seconds: 0.70\n",
            "Epoch  15/25 Batch  125/276 - Loss:  2.013, Seconds: 0.71\n",
            "Epoch  15/25 Batch  130/276 - Loss:  2.059, Seconds: 0.72\n",
            "Epoch  15/25 Batch  135/276 - Loss:  2.001, Seconds: 0.69\n",
            "Epoch  15/25 Batch  140/276 - Loss:  1.933, Seconds: 0.74\n",
            "Epoch  15/25 Batch  145/276 - Loss:  1.992, Seconds: 0.73\n",
            "Epoch  15/25 Batch  150/276 - Loss:  2.052, Seconds: 0.68\n",
            "Epoch  15/25 Batch  155/276 - Loss:  2.058, Seconds: 0.70\n",
            "Epoch  15/25 Batch  160/276 - Loss:  2.025, Seconds: 0.73\n",
            "Epoch  15/25 Batch  165/276 - Loss:  1.979, Seconds: 0.71\n",
            "Epoch  15/25 Batch  170/276 - Loss:  1.944, Seconds: 0.71\n",
            "Epoch  15/25 Batch  175/276 - Loss:  2.061, Seconds: 0.70\n",
            "Epoch  15/25 Batch  180/276 - Loss:  2.034, Seconds: 0.78\n",
            "Average loss for this update: 2.01\n",
            "New Record!\n",
            "Epoch  15/25 Batch  185/276 - Loss:  2.034, Seconds: 0.72\n",
            "Epoch  15/25 Batch  190/276 - Loss:  2.014, Seconds: 0.73\n",
            "Epoch  15/25 Batch  195/276 - Loss:  2.008, Seconds: 0.74\n",
            "Epoch  15/25 Batch  200/276 - Loss:  2.047, Seconds: 0.75\n",
            "Epoch  15/25 Batch  205/276 - Loss:  2.105, Seconds: 0.73\n",
            "Epoch  15/25 Batch  210/276 - Loss:  1.993, Seconds: 0.78\n",
            "Epoch  15/25 Batch  215/276 - Loss:  1.999, Seconds: 0.73\n",
            "Epoch  15/25 Batch  220/276 - Loss:  2.054, Seconds: 0.73\n",
            "Epoch  15/25 Batch  225/276 - Loss:  1.976, Seconds: 0.74\n",
            "Epoch  15/25 Batch  230/276 - Loss:  1.966, Seconds: 0.82\n",
            "Epoch  15/25 Batch  235/276 - Loss:  1.960, Seconds: 0.76\n",
            "Epoch  15/25 Batch  240/276 - Loss:  1.972, Seconds: 0.76\n",
            "Epoch  15/25 Batch  245/276 - Loss:  1.983, Seconds: 0.75\n",
            "Epoch  15/25 Batch  250/276 - Loss:  1.980, Seconds: 0.81\n",
            "Epoch  15/25 Batch  255/276 - Loss:  2.095, Seconds: 0.80\n",
            "Epoch  15/25 Batch  260/276 - Loss:  2.010, Seconds: 0.80\n",
            "Epoch  15/25 Batch  265/276 - Loss:  2.093, Seconds: 0.78\n",
            "Epoch  15/25 Batch  270/276 - Loss:  2.074, Seconds: 0.78\n",
            "Average loss for this update: 2.025\n",
            "No Improvement.\n",
            "Epoch  15/25 Batch  275/276 - Loss:  2.124, Seconds: 0.98\n",
            "Epoch  16/25 Batch    5/276 - Loss:  2.879, Seconds: 0.58\n",
            "Epoch  16/25 Batch   10/276 - Loss:  2.280, Seconds: 0.61\n",
            "Epoch  16/25 Batch   15/276 - Loss:  2.248, Seconds: 0.62\n",
            "Epoch  16/25 Batch   20/276 - Loss:  2.244, Seconds: 0.62\n",
            "Epoch  16/25 Batch   25/276 - Loss:  2.203, Seconds: 0.68\n",
            "Epoch  16/25 Batch   30/276 - Loss:  2.160, Seconds: 0.64\n",
            "Epoch  16/25 Batch   35/276 - Loss:  2.132, Seconds: 0.64\n",
            "Epoch  16/25 Batch   40/276 - Loss:  2.081, Seconds: 0.64\n",
            "Epoch  16/25 Batch   45/276 - Loss:  1.985, Seconds: 0.62\n",
            "Epoch  16/25 Batch   50/276 - Loss:  2.087, Seconds: 0.65\n",
            "Epoch  16/25 Batch   55/276 - Loss:  2.117, Seconds: 0.65\n",
            "Epoch  16/25 Batch   60/276 - Loss:  2.022, Seconds: 0.64\n",
            "Epoch  16/25 Batch   65/276 - Loss:  1.957, Seconds: 0.67\n",
            "Epoch  16/25 Batch   70/276 - Loss:  2.021, Seconds: 0.66\n",
            "Epoch  16/25 Batch   75/276 - Loss:  2.095, Seconds: 0.66\n",
            "Epoch  16/25 Batch   80/276 - Loss:  1.976, Seconds: 0.69\n",
            "Epoch  16/25 Batch   85/276 - Loss:  1.985, Seconds: 0.66\n",
            "Epoch  16/25 Batch   90/276 - Loss:  1.963, Seconds: 0.66\n",
            "Average loss for this update: 2.133\n",
            "No Improvement.\n",
            "Epoch  16/25 Batch   95/276 - Loss:  2.004, Seconds: 0.69\n",
            "Epoch  16/25 Batch  100/276 - Loss:  1.923, Seconds: 0.66\n",
            "Epoch  16/25 Batch  105/276 - Loss:  1.962, Seconds: 0.65\n",
            "Epoch  16/25 Batch  110/276 - Loss:  1.884, Seconds: 0.66\n",
            "Epoch  16/25 Batch  115/276 - Loss:  1.912, Seconds: 0.65\n",
            "Epoch  16/25 Batch  120/276 - Loss:  1.963, Seconds: 0.77\n",
            "Epoch  16/25 Batch  125/276 - Loss:  1.971, Seconds: 0.69\n",
            "Epoch  16/25 Batch  130/276 - Loss:  2.011, Seconds: 0.73\n",
            "Epoch  16/25 Batch  135/276 - Loss:  2.042, Seconds: 0.68\n",
            "Epoch  16/25 Batch  140/276 - Loss:  1.915, Seconds: 0.67\n",
            "Epoch  16/25 Batch  145/276 - Loss:  2.007, Seconds: 0.78\n",
            "Epoch  16/25 Batch  150/276 - Loss:  2.062, Seconds: 0.71\n",
            "Epoch  16/25 Batch  155/276 - Loss:  2.052, Seconds: 0.71\n",
            "Epoch  16/25 Batch  160/276 - Loss:  2.020, Seconds: 0.73\n",
            "Epoch  16/25 Batch  165/276 - Loss:  1.956, Seconds: 0.73\n",
            "Epoch  16/25 Batch  170/276 - Loss:  1.933, Seconds: 0.72\n",
            "Epoch  16/25 Batch  175/276 - Loss:  1.997, Seconds: 0.72\n",
            "Epoch  16/25 Batch  180/276 - Loss:  2.062, Seconds: 0.75\n",
            "Average loss for this update: 1.982\n",
            "New Record!\n",
            "Epoch  16/25 Batch  185/276 - Loss:  1.991, Seconds: 0.72\n",
            "Epoch  16/25 Batch  190/276 - Loss:  1.980, Seconds: 0.72\n",
            "Epoch  16/25 Batch  195/276 - Loss:  2.056, Seconds: 0.74\n",
            "Epoch  16/25 Batch  200/276 - Loss:  2.229, Seconds: 0.71\n",
            "Epoch  16/25 Batch  205/276 - Loss:  2.231, Seconds: 0.72\n",
            "Epoch  16/25 Batch  210/276 - Loss:  2.087, Seconds: 0.82\n",
            "Epoch  16/25 Batch  215/276 - Loss:  2.013, Seconds: 0.75\n",
            "Epoch  16/25 Batch  220/276 - Loss:  2.101, Seconds: 0.80\n",
            "Epoch  16/25 Batch  225/276 - Loss:  2.035, Seconds: 0.77\n",
            "Epoch  16/25 Batch  230/276 - Loss:  1.996, Seconds: 0.78\n",
            "Epoch  16/25 Batch  235/276 - Loss:  2.070, Seconds: 0.79\n",
            "Epoch  16/25 Batch  240/276 - Loss:  2.002, Seconds: 0.76\n",
            "Epoch  16/25 Batch  245/276 - Loss:  1.980, Seconds: 0.76\n",
            "Epoch  16/25 Batch  250/276 - Loss:  2.027, Seconds: 0.77\n",
            "Epoch  16/25 Batch  255/276 - Loss:  2.079, Seconds: 0.80\n",
            "Epoch  16/25 Batch  260/276 - Loss:  1.987, Seconds: 0.78\n",
            "Epoch  16/25 Batch  265/276 - Loss:  2.119, Seconds: 0.81\n",
            "Epoch  16/25 Batch  270/276 - Loss:  2.084, Seconds: 0.86\n",
            "Average loss for this update: 2.064\n",
            "No Improvement.\n",
            "Epoch  16/25 Batch  275/276 - Loss:  2.164, Seconds: 0.92\n",
            "Epoch  17/25 Batch    5/276 - Loss:  2.760, Seconds: 0.59\n",
            "Epoch  17/25 Batch   10/276 - Loss:  2.228, Seconds: 0.63\n",
            "Epoch  17/25 Batch   15/276 - Loss:  2.186, Seconds: 0.67\n",
            "Epoch  17/25 Batch   20/276 - Loss:  2.219, Seconds: 0.60\n",
            "Epoch  17/25 Batch   25/276 - Loss:  2.155, Seconds: 0.65\n",
            "Epoch  17/25 Batch   30/276 - Loss:  2.134, Seconds: 0.65\n",
            "Epoch  17/25 Batch   35/276 - Loss:  2.132, Seconds: 0.66\n",
            "Epoch  17/25 Batch   40/276 - Loss:  2.048, Seconds: 0.64\n",
            "Epoch  17/25 Batch   45/276 - Loss:  1.980, Seconds: 0.71\n",
            "Epoch  17/25 Batch   50/276 - Loss:  2.089, Seconds: 0.67\n",
            "Epoch  17/25 Batch   55/276 - Loss:  2.074, Seconds: 0.67\n",
            "Epoch  17/25 Batch   60/276 - Loss:  1.953, Seconds: 0.63\n",
            "Epoch  17/25 Batch   65/276 - Loss:  1.904, Seconds: 0.65\n",
            "Epoch  17/25 Batch   70/276 - Loss:  2.001, Seconds: 0.69\n",
            "Epoch  17/25 Batch   75/276 - Loss:  2.034, Seconds: 0.67\n",
            "Epoch  17/25 Batch   80/276 - Loss:  1.950, Seconds: 0.66\n",
            "Epoch  17/25 Batch   85/276 - Loss:  1.937, Seconds: 0.71\n",
            "Epoch  17/25 Batch   90/276 - Loss:  1.921, Seconds: 0.66\n",
            "Average loss for this update: 2.092\n",
            "No Improvement.\n",
            "Epoch  17/25 Batch   95/276 - Loss:  1.976, Seconds: 0.65\n",
            "Epoch  17/25 Batch  100/276 - Loss:  1.916, Seconds: 0.71\n",
            "Epoch  17/25 Batch  105/276 - Loss:  1.981, Seconds: 0.66\n",
            "Epoch  17/25 Batch  110/276 - Loss:  1.888, Seconds: 0.65\n",
            "Epoch  17/25 Batch  115/276 - Loss:  1.904, Seconds: 0.72\n",
            "Epoch  17/25 Batch  120/276 - Loss:  1.945, Seconds: 0.68\n",
            "Epoch  17/25 Batch  125/276 - Loss:  1.907, Seconds: 0.65\n",
            "Epoch  17/25 Batch  130/276 - Loss:  1.956, Seconds: 0.74\n",
            "Epoch  17/25 Batch  135/276 - Loss:  1.937, Seconds: 0.69\n",
            "Epoch  17/25 Batch  140/276 - Loss:  1.815, Seconds: 0.66\n",
            "Epoch  17/25 Batch  145/276 - Loss:  1.890, Seconds: 0.68\n",
            "Epoch  17/25 Batch  150/276 - Loss:  2.020, Seconds: 0.70\n",
            "Epoch  17/25 Batch  155/276 - Loss:  1.996, Seconds: 0.70\n",
            "Epoch  17/25 Batch  160/276 - Loss:  1.913, Seconds: 0.69\n",
            "Epoch  17/25 Batch  165/276 - Loss:  1.844, Seconds: 0.69\n",
            "Epoch  17/25 Batch  170/276 - Loss:  1.860, Seconds: 0.70\n",
            "Epoch  17/25 Batch  175/276 - Loss:  1.925, Seconds: 0.69\n",
            "Epoch  17/25 Batch  180/276 - Loss:  1.896, Seconds: 0.75\n",
            "Average loss for this update: 1.92\n",
            "New Record!\n",
            "Epoch  17/25 Batch  185/276 - Loss:  1.896, Seconds: 0.76\n",
            "Epoch  17/25 Batch  190/276 - Loss:  1.862, Seconds: 0.70\n",
            "Epoch  17/25 Batch  195/276 - Loss:  1.891, Seconds: 0.75\n",
            "Epoch  17/25 Batch  200/276 - Loss:  1.985, Seconds: 0.73\n",
            "Epoch  17/25 Batch  205/276 - Loss:  1.929, Seconds: 0.70\n",
            "Epoch  17/25 Batch  210/276 - Loss:  1.928, Seconds: 0.73\n",
            "Epoch  17/25 Batch  215/276 - Loss:  1.903, Seconds: 0.73\n",
            "Epoch  17/25 Batch  220/276 - Loss:  1.923, Seconds: 0.73\n",
            "Epoch  17/25 Batch  225/276 - Loss:  1.877, Seconds: 0.72\n",
            "Epoch  17/25 Batch  230/276 - Loss:  1.850, Seconds: 0.76\n",
            "Epoch  17/25 Batch  235/276 - Loss:  1.897, Seconds: 0.74\n",
            "Epoch  17/25 Batch  240/276 - Loss:  1.841, Seconds: 0.78\n",
            "Epoch  17/25 Batch  245/276 - Loss:  1.900, Seconds: 0.73\n",
            "Epoch  17/25 Batch  250/276 - Loss:  1.894, Seconds: 0.77\n",
            "Epoch  17/25 Batch  255/276 - Loss:  1.968, Seconds: 0.77\n",
            "Epoch  17/25 Batch  260/276 - Loss:  1.861, Seconds: 0.77\n",
            "Epoch  17/25 Batch  265/276 - Loss:  1.960, Seconds: 0.79\n",
            "Epoch  17/25 Batch  270/276 - Loss:  1.968, Seconds: 0.78\n",
            "Average loss for this update: 1.91\n",
            "New Record!\n",
            "Epoch  17/25 Batch  275/276 - Loss:  2.007, Seconds: 0.92\n",
            "Epoch  18/25 Batch    5/276 - Loss:  2.579, Seconds: 0.59\n",
            "Epoch  18/25 Batch   10/276 - Loss:  2.056, Seconds: 0.59\n",
            "Epoch  18/25 Batch   15/276 - Loss:  2.081, Seconds: 0.59\n",
            "Epoch  18/25 Batch   20/276 - Loss:  2.125, Seconds: 0.63\n",
            "Epoch  18/25 Batch   25/276 - Loss:  2.027, Seconds: 0.63\n",
            "Epoch  18/25 Batch   30/276 - Loss:  2.003, Seconds: 0.62\n",
            "Epoch  18/25 Batch   35/276 - Loss:  2.031, Seconds: 0.79\n",
            "Epoch  18/25 Batch   40/276 - Loss:  1.921, Seconds: 0.62\n",
            "Epoch  18/25 Batch   45/276 - Loss:  1.873, Seconds: 0.64\n",
            "Epoch  18/25 Batch   50/276 - Loss:  1.992, Seconds: 0.67\n",
            "Epoch  18/25 Batch   55/276 - Loss:  1.958, Seconds: 0.62\n",
            "Epoch  18/25 Batch   60/276 - Loss:  1.834, Seconds: 0.62\n",
            "Epoch  18/25 Batch   65/276 - Loss:  1.831, Seconds: 0.63\n",
            "Epoch  18/25 Batch   70/276 - Loss:  1.893, Seconds: 0.64\n",
            "Epoch  18/25 Batch   75/276 - Loss:  1.950, Seconds: 0.67\n",
            "Epoch  18/25 Batch   80/276 - Loss:  1.847, Seconds: 0.66\n",
            "Epoch  18/25 Batch   85/276 - Loss:  1.838, Seconds: 0.64\n",
            "Epoch  18/25 Batch   90/276 - Loss:  1.873, Seconds: 0.69\n",
            "Average loss for this update: 1.982\n",
            "No Improvement.\n",
            "Epoch  18/25 Batch   95/276 - Loss:  1.899, Seconds: 0.69\n",
            "Epoch  18/25 Batch  100/276 - Loss:  1.840, Seconds: 0.67\n",
            "Epoch  18/25 Batch  105/276 - Loss:  1.887, Seconds: 0.69\n",
            "Epoch  18/25 Batch  110/276 - Loss:  1.792, Seconds: 0.65\n",
            "Epoch  18/25 Batch  115/276 - Loss:  1.789, Seconds: 0.74\n",
            "Epoch  18/25 Batch  120/276 - Loss:  1.867, Seconds: 0.70\n",
            "Epoch  18/25 Batch  125/276 - Loss:  1.830, Seconds: 0.71\n",
            "Epoch  18/25 Batch  130/276 - Loss:  1.904, Seconds: 0.69\n",
            "Epoch  18/25 Batch  135/276 - Loss:  1.825, Seconds: 0.75\n",
            "Epoch  18/25 Batch  140/276 - Loss:  1.723, Seconds: 0.76\n",
            "Epoch  18/25 Batch  145/276 - Loss:  1.846, Seconds: 0.68\n",
            "Epoch  18/25 Batch  150/276 - Loss:  1.939, Seconds: 0.70\n",
            "Epoch  18/25 Batch  155/276 - Loss:  1.900, Seconds: 0.74\n",
            "Epoch  18/25 Batch  160/276 - Loss:  1.871, Seconds: 0.70\n",
            "Epoch  18/25 Batch  165/276 - Loss:  1.818, Seconds: 0.70\n",
            "Epoch  18/25 Batch  170/276 - Loss:  1.827, Seconds: 0.70\n",
            "Epoch  18/25 Batch  175/276 - Loss:  1.855, Seconds: 0.71\n",
            "Epoch  18/25 Batch  180/276 - Loss:  1.897, Seconds: 0.74\n",
            "Average loss for this update: 1.852\n",
            "New Record!\n",
            "Epoch  18/25 Batch  185/276 - Loss:  1.890, Seconds: 0.77\n",
            "Epoch  18/25 Batch  190/276 - Loss:  1.846, Seconds: 0.69\n",
            "Epoch  18/25 Batch  195/276 - Loss:  1.855, Seconds: 0.73\n",
            "Epoch  18/25 Batch  200/276 - Loss:  1.928, Seconds: 0.72\n",
            "Epoch  18/25 Batch  205/276 - Loss:  1.920, Seconds: 0.73\n",
            "Epoch  18/25 Batch  210/276 - Loss:  1.883, Seconds: 0.74\n",
            "Epoch  18/25 Batch  215/276 - Loss:  1.812, Seconds: 0.72\n",
            "Epoch  18/25 Batch  220/276 - Loss:  1.914, Seconds: 0.78\n",
            "Epoch  18/25 Batch  225/276 - Loss:  1.792, Seconds: 0.75\n",
            "Epoch  18/25 Batch  230/276 - Loss:  1.758, Seconds: 0.74\n",
            "Epoch  18/25 Batch  235/276 - Loss:  1.888, Seconds: 0.76\n",
            "Epoch  18/25 Batch  240/276 - Loss:  1.794, Seconds: 0.82\n",
            "Epoch  18/25 Batch  245/276 - Loss:  1.837, Seconds: 0.76\n",
            "Epoch  18/25 Batch  250/276 - Loss:  1.839, Seconds: 0.77\n",
            "Epoch  18/25 Batch  255/276 - Loss:  1.873, Seconds: 0.76\n",
            "Epoch  18/25 Batch  260/276 - Loss:  1.830, Seconds: 0.82\n",
            "Epoch  18/25 Batch  265/276 - Loss:  1.898, Seconds: 0.77\n",
            "Epoch  18/25 Batch  270/276 - Loss:  1.843, Seconds: 0.79\n",
            "Average loss for this update: 1.856\n",
            "No Improvement.\n",
            "Epoch  18/25 Batch  275/276 - Loss:  1.922, Seconds: 0.88\n",
            "Epoch  19/25 Batch    5/276 - Loss:  2.541, Seconds: 0.64\n",
            "Epoch  19/25 Batch   10/276 - Loss:  1.975, Seconds: 0.60\n",
            "Epoch  19/25 Batch   15/276 - Loss:  1.972, Seconds: 0.62\n",
            "Epoch  19/25 Batch   20/276 - Loss:  2.076, Seconds: 0.66\n",
            "Epoch  19/25 Batch   25/276 - Loss:  2.016, Seconds: 0.62\n",
            "Epoch  19/25 Batch   30/276 - Loss:  1.938, Seconds: 0.62\n",
            "Epoch  19/25 Batch   35/276 - Loss:  1.986, Seconds: 0.64\n",
            "Epoch  19/25 Batch   40/276 - Loss:  1.871, Seconds: 0.62\n",
            "Epoch  19/25 Batch   45/276 - Loss:  1.860, Seconds: 0.68\n",
            "Epoch  19/25 Batch   50/276 - Loss:  1.939, Seconds: 0.64\n",
            "Epoch  19/25 Batch   55/276 - Loss:  1.919, Seconds: 0.64\n",
            "Epoch  19/25 Batch   60/276 - Loss:  1.836, Seconds: 0.67\n",
            "Epoch  19/25 Batch   65/276 - Loss:  1.786, Seconds: 0.64\n",
            "Epoch  19/25 Batch   70/276 - Loss:  1.910, Seconds: 0.68\n",
            "Epoch  19/25 Batch   75/276 - Loss:  1.891, Seconds: 0.68\n",
            "Epoch  19/25 Batch   80/276 - Loss:  1.845, Seconds: 0.67\n",
            "Epoch  19/25 Batch   85/276 - Loss:  1.850, Seconds: 0.67\n",
            "Epoch  19/25 Batch   90/276 - Loss:  1.830, Seconds: 0.65\n",
            "Average loss for this update: 1.944\n",
            "No Improvement.\n",
            "Epoch  19/25 Batch   95/276 - Loss:  1.832, Seconds: 0.67\n",
            "Epoch  19/25 Batch  100/276 - Loss:  1.816, Seconds: 0.66\n",
            "Epoch  19/25 Batch  105/276 - Loss:  1.820, Seconds: 0.68\n",
            "Epoch  19/25 Batch  110/276 - Loss:  1.761, Seconds: 0.65\n",
            "Epoch  19/25 Batch  115/276 - Loss:  1.752, Seconds: 0.69\n",
            "Epoch  19/25 Batch  120/276 - Loss:  1.801, Seconds: 0.71\n",
            "Epoch  19/25 Batch  125/276 - Loss:  1.820, Seconds: 0.67\n",
            "Epoch  19/25 Batch  130/276 - Loss:  1.812, Seconds: 0.70\n",
            "Epoch  19/25 Batch  135/276 - Loss:  1.838, Seconds: 0.72\n",
            "Epoch  19/25 Batch  140/276 - Loss:  1.707, Seconds: 0.72\n",
            "Epoch  19/25 Batch  145/276 - Loss:  1.831, Seconds: 0.68\n",
            "Epoch  19/25 Batch  150/276 - Loss:  1.903, Seconds: 0.73\n",
            "Epoch  19/25 Batch  155/276 - Loss:  1.843, Seconds: 0.70\n",
            "Epoch  19/25 Batch  160/276 - Loss:  1.845, Seconds: 0.70\n",
            "Epoch  19/25 Batch  165/276 - Loss:  1.765, Seconds: 0.71\n",
            "Epoch  19/25 Batch  170/276 - Loss:  1.807, Seconds: 0.72\n",
            "Epoch  19/25 Batch  175/276 - Loss:  1.831, Seconds: 0.70\n",
            "Epoch  19/25 Batch  180/276 - Loss:  1.785, Seconds: 0.72\n",
            "Average loss for this update: 1.81\n",
            "New Record!\n",
            "Epoch  19/25 Batch  185/276 - Loss:  1.804, Seconds: 0.74\n",
            "Epoch  19/25 Batch  190/276 - Loss:  1.784, Seconds: 0.69\n",
            "Epoch  19/25 Batch  195/276 - Loss:  1.789, Seconds: 0.71\n",
            "Epoch  19/25 Batch  200/276 - Loss:  1.846, Seconds: 0.77\n",
            "Epoch  19/25 Batch  205/276 - Loss:  1.846, Seconds: 0.73\n",
            "Epoch  19/25 Batch  210/276 - Loss:  1.788, Seconds: 0.72\n",
            "Epoch  19/25 Batch  215/276 - Loss:  1.786, Seconds: 0.73\n",
            "Epoch  19/25 Batch  220/276 - Loss:  1.842, Seconds: 0.74\n",
            "Epoch  19/25 Batch  225/276 - Loss:  1.774, Seconds: 0.73\n",
            "Epoch  19/25 Batch  230/276 - Loss:  1.711, Seconds: 0.73\n",
            "Epoch  19/25 Batch  235/276 - Loss:  1.824, Seconds: 0.75\n",
            "Epoch  19/25 Batch  240/276 - Loss:  1.740, Seconds: 0.80\n",
            "Epoch  19/25 Batch  245/276 - Loss:  1.812, Seconds: 0.75\n",
            "Epoch  19/25 Batch  250/276 - Loss:  1.772, Seconds: 0.78\n",
            "Epoch  19/25 Batch  255/276 - Loss:  1.862, Seconds: 0.76\n",
            "Epoch  19/25 Batch  260/276 - Loss:  1.825, Seconds: 0.82\n",
            "Epoch  19/25 Batch  265/276 - Loss:  1.842, Seconds: 0.78\n",
            "Epoch  19/25 Batch  270/276 - Loss:  1.822, Seconds: 0.78\n",
            "Average loss for this update: 1.808\n",
            "New Record!\n",
            "Epoch  19/25 Batch  275/276 - Loss:  1.906, Seconds: 0.91\n",
            "Epoch  20/25 Batch    5/276 - Loss:  2.521, Seconds: 0.57\n",
            "Epoch  20/25 Batch   10/276 - Loss:  1.956, Seconds: 0.58\n",
            "Epoch  20/25 Batch   15/276 - Loss:  1.951, Seconds: 0.63\n",
            "Epoch  20/25 Batch   20/276 - Loss:  1.979, Seconds: 0.61\n",
            "Epoch  20/25 Batch   25/276 - Loss:  1.975, Seconds: 0.62\n",
            "Epoch  20/25 Batch   30/276 - Loss:  1.881, Seconds: 0.61\n",
            "Epoch  20/25 Batch   35/276 - Loss:  1.880, Seconds: 0.62\n",
            "Epoch  20/25 Batch   40/276 - Loss:  1.815, Seconds: 0.68\n",
            "Epoch  20/25 Batch   45/276 - Loss:  1.804, Seconds: 0.62\n",
            "Epoch  20/25 Batch   50/276 - Loss:  1.861, Seconds: 0.67\n",
            "Epoch  20/25 Batch   55/276 - Loss:  1.882, Seconds: 0.69\n",
            "Epoch  20/25 Batch   60/276 - Loss:  1.762, Seconds: 0.63\n",
            "Epoch  20/25 Batch   65/276 - Loss:  1.748, Seconds: 0.65\n",
            "Epoch  20/25 Batch   70/276 - Loss:  1.844, Seconds: 0.67\n",
            "Epoch  20/25 Batch   75/276 - Loss:  1.894, Seconds: 0.66\n",
            "Epoch  20/25 Batch   80/276 - Loss:  1.809, Seconds: 0.67\n",
            "Epoch  20/25 Batch   85/276 - Loss:  1.796, Seconds: 0.66\n",
            "Epoch  20/25 Batch   90/276 - Loss:  1.772, Seconds: 0.66\n",
            "Average loss for this update: 1.894\n",
            "No Improvement.\n",
            "Epoch  20/25 Batch   95/276 - Loss:  1.768, Seconds: 0.66\n",
            "Epoch  20/25 Batch  100/276 - Loss:  1.752, Seconds: 0.66\n",
            "Epoch  20/25 Batch  105/276 - Loss:  1.794, Seconds: 0.67\n",
            "Epoch  20/25 Batch  110/276 - Loss:  1.711, Seconds: 0.64\n",
            "Epoch  20/25 Batch  115/276 - Loss:  1.737, Seconds: 0.66\n",
            "Epoch  20/25 Batch  120/276 - Loss:  1.805, Seconds: 0.67\n",
            "Epoch  20/25 Batch  125/276 - Loss:  1.807, Seconds: 0.69\n",
            "Epoch  20/25 Batch  130/276 - Loss:  1.792, Seconds: 0.77\n",
            "Epoch  20/25 Batch  135/276 - Loss:  1.787, Seconds: 0.67\n",
            "Epoch  20/25 Batch  140/276 - Loss:  1.676, Seconds: 0.68\n",
            "Epoch  20/25 Batch  145/276 - Loss:  1.764, Seconds: 0.76\n",
            "Epoch  20/25 Batch  150/276 - Loss:  1.867, Seconds: 0.70\n",
            "Epoch  20/25 Batch  155/276 - Loss:  1.804, Seconds: 0.69\n",
            "Epoch  20/25 Batch  160/276 - Loss:  1.770, Seconds: 0.69\n",
            "Epoch  20/25 Batch  165/276 - Loss:  1.750, Seconds: 0.72\n",
            "Epoch  20/25 Batch  170/276 - Loss:  1.752, Seconds: 0.70\n",
            "Epoch  20/25 Batch  175/276 - Loss:  1.806, Seconds: 0.70\n",
            "Epoch  20/25 Batch  180/276 - Loss:  1.765, Seconds: 0.79\n",
            "Average loss for this update: 1.773\n",
            "New Record!\n",
            "Epoch  20/25 Batch  185/276 - Loss:  1.770, Seconds: 0.74\n",
            "Epoch  20/25 Batch  190/276 - Loss:  1.757, Seconds: 0.69\n",
            "Epoch  20/25 Batch  195/276 - Loss:  1.800, Seconds: 0.78\n",
            "Epoch  20/25 Batch  200/276 - Loss:  1.828, Seconds: 0.73\n",
            "Epoch  20/25 Batch  205/276 - Loss:  1.835, Seconds: 0.73\n",
            "Epoch  20/25 Batch  210/276 - Loss:  1.799, Seconds: 0.70\n",
            "Epoch  20/25 Batch  215/276 - Loss:  1.735, Seconds: 0.73\n",
            "Epoch  20/25 Batch  220/276 - Loss:  1.805, Seconds: 0.72\n",
            "Epoch  20/25 Batch  225/276 - Loss:  1.765, Seconds: 0.74\n",
            "Epoch  20/25 Batch  230/276 - Loss:  1.722, Seconds: 0.73\n",
            "Epoch  20/25 Batch  235/276 - Loss:  1.815, Seconds: 0.71\n",
            "Epoch  20/25 Batch  240/276 - Loss:  1.749, Seconds: 0.75\n",
            "Epoch  20/25 Batch  245/276 - Loss:  1.777, Seconds: 0.74\n",
            "Epoch  20/25 Batch  250/276 - Loss:  1.712, Seconds: 0.77\n",
            "Epoch  20/25 Batch  255/276 - Loss:  1.791, Seconds: 0.75\n",
            "Epoch  20/25 Batch  260/276 - Loss:  1.786, Seconds: 0.76\n",
            "Epoch  20/25 Batch  265/276 - Loss:  1.843, Seconds: 0.79\n",
            "Epoch  20/25 Batch  270/276 - Loss:  1.790, Seconds: 0.78\n",
            "Average loss for this update: 1.785\n",
            "No Improvement.\n",
            "Epoch  20/25 Batch  275/276 - Loss:  1.881, Seconds: 0.94\n",
            "Epoch  21/25 Batch    5/276 - Loss:  2.476, Seconds: 0.59\n",
            "Epoch  21/25 Batch   10/276 - Loss:  1.928, Seconds: 0.59\n",
            "Epoch  21/25 Batch   15/276 - Loss:  1.930, Seconds: 0.59\n",
            "Epoch  21/25 Batch   20/276 - Loss:  1.962, Seconds: 0.63\n",
            "Epoch  21/25 Batch   25/276 - Loss:  1.919, Seconds: 0.71\n",
            "Epoch  21/25 Batch   30/276 - Loss:  1.861, Seconds: 0.63\n",
            "Epoch  21/25 Batch   35/276 - Loss:  1.922, Seconds: 0.65\n",
            "Epoch  21/25 Batch   40/276 - Loss:  1.782, Seconds: 0.64\n",
            "Epoch  21/25 Batch   45/276 - Loss:  1.806, Seconds: 0.62\n",
            "Epoch  21/25 Batch   50/276 - Loss:  1.856, Seconds: 0.65\n",
            "Epoch  21/25 Batch   55/276 - Loss:  1.887, Seconds: 0.67\n",
            "Epoch  21/25 Batch   60/276 - Loss:  1.758, Seconds: 0.62\n",
            "Epoch  21/25 Batch   65/276 - Loss:  1.735, Seconds: 0.64\n",
            "Epoch  21/25 Batch   70/276 - Loss:  1.807, Seconds: 0.66\n",
            "Epoch  21/25 Batch   75/276 - Loss:  1.898, Seconds: 0.66\n",
            "Epoch  21/25 Batch   80/276 - Loss:  1.760, Seconds: 0.67\n",
            "Epoch  21/25 Batch   85/276 - Loss:  1.753, Seconds: 0.66\n",
            "Epoch  21/25 Batch   90/276 - Loss:  1.764, Seconds: 0.66\n",
            "Average loss for this update: 1.876\n",
            "No Improvement.\n",
            "Epoch  21/25 Batch   95/276 - Loss:  1.783, Seconds: 0.67\n",
            "Epoch  21/25 Batch  100/276 - Loss:  1.753, Seconds: 0.69\n",
            "Epoch  21/25 Batch  105/276 - Loss:  1.797, Seconds: 0.69\n",
            "Epoch  21/25 Batch  110/276 - Loss:  1.751, Seconds: 0.63\n",
            "Epoch  21/25 Batch  115/276 - Loss:  1.711, Seconds: 0.68\n",
            "Epoch  21/25 Batch  120/276 - Loss:  1.751, Seconds: 0.69\n",
            "Epoch  21/25 Batch  125/276 - Loss:  1.767, Seconds: 0.70\n",
            "Epoch  21/25 Batch  130/276 - Loss:  1.789, Seconds: 0.73\n",
            "Epoch  21/25 Batch  135/276 - Loss:  1.739, Seconds: 0.67\n",
            "Epoch  21/25 Batch  140/276 - Loss:  1.640, Seconds: 0.68\n",
            "Epoch  21/25 Batch  145/276 - Loss:  1.683, Seconds: 0.75\n",
            "Epoch  21/25 Batch  150/276 - Loss:  1.860, Seconds: 0.70\n",
            "Epoch  21/25 Batch  155/276 - Loss:  1.781, Seconds: 0.72\n",
            "Epoch  21/25 Batch  160/276 - Loss:  1.733, Seconds: 0.68\n",
            "Epoch  21/25 Batch  165/276 - Loss:  1.712, Seconds: 0.70\n",
            "Epoch  21/25 Batch  170/276 - Loss:  1.694, Seconds: 0.71\n",
            "Epoch  21/25 Batch  175/276 - Loss:  1.766, Seconds: 0.69\n",
            "Epoch  21/25 Batch  180/276 - Loss:  1.768, Seconds: 0.69\n",
            "Average loss for this update: 1.749\n",
            "New Record!\n",
            "Epoch  21/25 Batch  185/276 - Loss:  1.754, Seconds: 0.71\n",
            "Epoch  21/25 Batch  190/276 - Loss:  1.769, Seconds: 0.68\n",
            "Epoch  21/25 Batch  195/276 - Loss:  1.753, Seconds: 0.70\n",
            "Epoch  21/25 Batch  200/276 - Loss:  1.763, Seconds: 0.73\n",
            "Epoch  21/25 Batch  205/276 - Loss:  1.823, Seconds: 0.72\n",
            "Epoch  21/25 Batch  210/276 - Loss:  1.798, Seconds: 0.79\n",
            "Epoch  21/25 Batch  215/276 - Loss:  1.755, Seconds: 0.70\n",
            "Epoch  21/25 Batch  220/276 - Loss:  1.728, Seconds: 0.73\n",
            "Epoch  21/25 Batch  225/276 - Loss:  1.753, Seconds: 0.75\n",
            "Epoch  21/25 Batch  230/276 - Loss:  1.650, Seconds: 0.77\n",
            "Epoch  21/25 Batch  235/276 - Loss:  1.763, Seconds: 0.73\n",
            "Epoch  21/25 Batch  240/276 - Loss:  1.699, Seconds: 0.80\n",
            "Epoch  21/25 Batch  245/276 - Loss:  1.729, Seconds: 0.76\n",
            "Epoch  21/25 Batch  250/276 - Loss:  1.719, Seconds: 0.81\n",
            "Epoch  21/25 Batch  255/276 - Loss:  1.768, Seconds: 0.75\n",
            "Epoch  21/25 Batch  260/276 - Loss:  1.780, Seconds: 0.76\n",
            "Epoch  21/25 Batch  265/276 - Loss:  1.814, Seconds: 0.81\n",
            "Epoch  21/25 Batch  270/276 - Loss:  1.747, Seconds: 0.79\n",
            "Average loss for this update: 1.757\n",
            "No Improvement.\n",
            "Epoch  21/25 Batch  275/276 - Loss:  1.842, Seconds: 0.91\n",
            "Epoch  22/25 Batch    5/276 - Loss:  2.450, Seconds: 0.56\n",
            "Epoch  22/25 Batch   10/276 - Loss:  1.934, Seconds: 0.58\n",
            "Epoch  22/25 Batch   15/276 - Loss:  1.885, Seconds: 0.63\n",
            "Epoch  22/25 Batch   20/276 - Loss:  1.956, Seconds: 0.62\n",
            "Epoch  22/25 Batch   25/276 - Loss:  1.895, Seconds: 0.64\n",
            "Epoch  22/25 Batch   30/276 - Loss:  1.795, Seconds: 0.66\n",
            "Epoch  22/25 Batch   35/276 - Loss:  1.837, Seconds: 0.63\n",
            "Epoch  22/25 Batch   40/276 - Loss:  1.802, Seconds: 0.66\n",
            "Epoch  22/25 Batch   45/276 - Loss:  1.720, Seconds: 0.66\n",
            "Epoch  22/25 Batch   50/276 - Loss:  1.804, Seconds: 0.67\n",
            "Epoch  22/25 Batch   55/276 - Loss:  1.804, Seconds: 0.77\n",
            "Epoch  22/25 Batch   60/276 - Loss:  1.748, Seconds: 0.60\n",
            "Epoch  22/25 Batch   65/276 - Loss:  1.723, Seconds: 0.64\n",
            "Epoch  22/25 Batch   70/276 - Loss:  1.768, Seconds: 0.70\n",
            "Epoch  22/25 Batch   75/276 - Loss:  1.810, Seconds: 0.65\n",
            "Epoch  22/25 Batch   80/276 - Loss:  1.743, Seconds: 0.68\n",
            "Epoch  22/25 Batch   85/276 - Loss:  1.695, Seconds: 0.66\n",
            "Epoch  22/25 Batch   90/276 - Loss:  1.706, Seconds: 0.65\n",
            "Average loss for this update: 1.835\n",
            "No Improvement.\n",
            "Epoch  22/25 Batch   95/276 - Loss:  1.737, Seconds: 0.64\n",
            "Epoch  22/25 Batch  100/276 - Loss:  1.711, Seconds: 0.66\n",
            "Epoch  22/25 Batch  105/276 - Loss:  1.727, Seconds: 0.67\n",
            "Epoch  22/25 Batch  110/276 - Loss:  1.634, Seconds: 0.63\n",
            "Epoch  22/25 Batch  115/276 - Loss:  1.644, Seconds: 0.66\n",
            "Epoch  22/25 Batch  120/276 - Loss:  1.727, Seconds: 0.70\n",
            "Epoch  22/25 Batch  125/276 - Loss:  1.672, Seconds: 0.67\n",
            "Epoch  22/25 Batch  130/276 - Loss:  1.705, Seconds: 0.76\n",
            "Epoch  22/25 Batch  135/276 - Loss:  1.685, Seconds: 0.68\n",
            "Epoch  22/25 Batch  140/276 - Loss:  1.651, Seconds: 0.67\n",
            "Epoch  22/25 Batch  145/276 - Loss:  1.711, Seconds: 0.69\n",
            "Epoch  22/25 Batch  150/276 - Loss:  1.795, Seconds: 0.68\n",
            "Epoch  22/25 Batch  155/276 - Loss:  1.751, Seconds: 0.70\n",
            "Epoch  22/25 Batch  160/276 - Loss:  1.692, Seconds: 0.73\n",
            "Epoch  22/25 Batch  165/276 - Loss:  1.682, Seconds: 0.68\n",
            "Epoch  22/25 Batch  170/276 - Loss:  1.680, Seconds: 0.72\n",
            "Epoch  22/25 Batch  175/276 - Loss:  1.698, Seconds: 0.70\n",
            "Epoch  22/25 Batch  180/276 - Loss:  1.727, Seconds: 0.71\n",
            "Average loss for this update: 1.703\n",
            "New Record!\n",
            "Epoch  22/25 Batch  185/276 - Loss:  1.718, Seconds: 0.73\n",
            "Epoch  22/25 Batch  190/276 - Loss:  1.697, Seconds: 0.69\n",
            "Epoch  22/25 Batch  195/276 - Loss:  1.710, Seconds: 0.73\n",
            "Epoch  22/25 Batch  200/276 - Loss:  1.726, Seconds: 0.73\n",
            "Epoch  22/25 Batch  205/276 - Loss:  1.789, Seconds: 0.71\n",
            "Epoch  22/25 Batch  210/276 - Loss:  1.691, Seconds: 0.72\n",
            "Epoch  22/25 Batch  215/276 - Loss:  1.733, Seconds: 0.72\n",
            "Epoch  22/25 Batch  220/276 - Loss:  1.714, Seconds: 0.72\n",
            "Epoch  22/25 Batch  225/276 - Loss:  1.709, Seconds: 0.76\n",
            "Epoch  22/25 Batch  230/276 - Loss:  1.638, Seconds: 0.78\n",
            "Epoch  22/25 Batch  235/276 - Loss:  1.702, Seconds: 0.74\n",
            "Epoch  22/25 Batch  240/276 - Loss:  1.687, Seconds: 0.78\n",
            "Epoch  22/25 Batch  245/276 - Loss:  1.717, Seconds: 0.75\n",
            "Epoch  22/25 Batch  250/276 - Loss:  1.704, Seconds: 0.81\n",
            "Epoch  22/25 Batch  255/276 - Loss:  1.761, Seconds: 0.75\n",
            "Epoch  22/25 Batch  260/276 - Loss:  1.742, Seconds: 0.77\n",
            "Epoch  22/25 Batch  265/276 - Loss:  1.783, Seconds: 0.80\n",
            "Epoch  22/25 Batch  270/276 - Loss:  1.752, Seconds: 0.78\n",
            "Average loss for this update: 1.722\n",
            "No Improvement.\n",
            "Epoch  22/25 Batch  275/276 - Loss:  1.790, Seconds: 0.93\n",
            "Epoch  23/25 Batch    5/276 - Loss:  2.384, Seconds: 0.58\n",
            "Epoch  23/25 Batch   10/276 - Loss:  1.875, Seconds: 0.57\n",
            "Epoch  23/25 Batch   15/276 - Loss:  1.907, Seconds: 0.62\n",
            "Epoch  23/25 Batch   20/276 - Loss:  1.874, Seconds: 0.61\n",
            "Epoch  23/25 Batch   25/276 - Loss:  1.897, Seconds: 0.62\n",
            "Epoch  23/25 Batch   30/276 - Loss:  1.768, Seconds: 0.62\n",
            "Epoch  23/25 Batch   35/276 - Loss:  1.828, Seconds: 0.65\n",
            "Epoch  23/25 Batch   40/276 - Loss:  1.723, Seconds: 0.68\n",
            "Epoch  23/25 Batch   45/276 - Loss:  1.703, Seconds: 0.63\n",
            "Epoch  23/25 Batch   50/276 - Loss:  1.768, Seconds: 0.63\n",
            "Epoch  23/25 Batch   55/276 - Loss:  1.804, Seconds: 0.63\n",
            "Epoch  23/25 Batch   60/276 - Loss:  1.717, Seconds: 0.64\n",
            "Epoch  23/25 Batch   65/276 - Loss:  1.678, Seconds: 0.62\n",
            "Epoch  23/25 Batch   70/276 - Loss:  1.749, Seconds: 0.64\n",
            "Epoch  23/25 Batch   75/276 - Loss:  1.811, Seconds: 0.69\n",
            "Epoch  23/25 Batch   80/276 - Loss:  1.694, Seconds: 0.65\n",
            "Epoch  23/25 Batch   85/276 - Loss:  1.703, Seconds: 0.66\n",
            "Epoch  23/25 Batch   90/276 - Loss:  1.686, Seconds: 0.67\n",
            "Average loss for this update: 1.807\n",
            "No Improvement.\n",
            "Epoch  23/25 Batch   95/276 - Loss:  1.687, Seconds: 0.65\n",
            "Epoch  23/25 Batch  100/276 - Loss:  1.659, Seconds: 0.70\n",
            "Epoch  23/25 Batch  105/276 - Loss:  1.669, Seconds: 0.67\n",
            "Epoch  23/25 Batch  110/276 - Loss:  1.646, Seconds: 0.66\n",
            "Epoch  23/25 Batch  115/276 - Loss:  1.636, Seconds: 0.65\n",
            "Epoch  23/25 Batch  120/276 - Loss:  1.658, Seconds: 0.69\n",
            "Epoch  23/25 Batch  125/276 - Loss:  1.748, Seconds: 0.67\n",
            "Epoch  23/25 Batch  130/276 - Loss:  1.653, Seconds: 0.68\n",
            "Epoch  23/25 Batch  135/276 - Loss:  1.665, Seconds: 0.70\n",
            "Epoch  23/25 Batch  140/276 - Loss:  1.607, Seconds: 0.65\n",
            "Epoch  23/25 Batch  145/276 - Loss:  1.660, Seconds: 0.70\n",
            "Epoch  23/25 Batch  150/276 - Loss:  1.737, Seconds: 0.69\n",
            "Epoch  23/25 Batch  155/276 - Loss:  1.727, Seconds: 0.69\n",
            "Epoch  23/25 Batch  160/276 - Loss:  1.671, Seconds: 0.73\n",
            "Epoch  23/25 Batch  165/276 - Loss:  1.650, Seconds: 0.72\n",
            "Epoch  23/25 Batch  170/276 - Loss:  1.665, Seconds: 0.70\n",
            "Epoch  23/25 Batch  175/276 - Loss:  1.707, Seconds: 0.69\n",
            "Epoch  23/25 Batch  180/276 - Loss:  1.736, Seconds: 0.72\n",
            "Average loss for this update: 1.677\n",
            "New Record!\n",
            "Epoch  23/25 Batch  185/276 - Loss:  1.690, Seconds: 0.76\n",
            "Epoch  23/25 Batch  190/276 - Loss:  1.639, Seconds: 0.70\n",
            "Epoch  23/25 Batch  195/276 - Loss:  1.638, Seconds: 0.72\n",
            "Epoch  23/25 Batch  200/276 - Loss:  1.732, Seconds: 0.73\n",
            "Epoch  23/25 Batch  205/276 - Loss:  1.750, Seconds: 0.72\n",
            "Epoch  23/25 Batch  210/276 - Loss:  1.709, Seconds: 0.78\n",
            "Epoch  23/25 Batch  215/276 - Loss:  1.695, Seconds: 0.73\n",
            "Epoch  23/25 Batch  220/276 - Loss:  1.711, Seconds: 0.75\n",
            "Epoch  23/25 Batch  225/276 - Loss:  1.695, Seconds: 0.75\n",
            "Epoch  23/25 Batch  230/276 - Loss:  1.610, Seconds: 0.87\n",
            "Epoch  23/25 Batch  235/276 - Loss:  1.706, Seconds: 0.76\n",
            "Epoch  23/25 Batch  240/276 - Loss:  1.627, Seconds: 0.75\n",
            "Epoch  23/25 Batch  245/276 - Loss:  1.650, Seconds: 0.75\n",
            "Epoch  23/25 Batch  250/276 - Loss:  1.644, Seconds: 0.80\n",
            "Epoch  23/25 Batch  255/276 - Loss:  1.731, Seconds: 0.76\n",
            "Epoch  23/25 Batch  260/276 - Loss:  1.673, Seconds: 0.76\n",
            "Epoch  23/25 Batch  265/276 - Loss:  1.775, Seconds: 0.76\n",
            "Epoch  23/25 Batch  270/276 - Loss:  1.722, Seconds: 0.87\n",
            "Average loss for this update: 1.691\n",
            "No Improvement.\n",
            "Epoch  23/25 Batch  275/276 - Loss:  1.769, Seconds: 0.87\n",
            "Epoch  24/25 Batch    5/276 - Loss:  2.317, Seconds: 0.57\n",
            "Epoch  24/25 Batch   10/276 - Loss:  1.838, Seconds: 0.59\n",
            "Epoch  24/25 Batch   15/276 - Loss:  1.840, Seconds: 0.61\n",
            "Epoch  24/25 Batch   20/276 - Loss:  1.876, Seconds: 0.61\n",
            "Epoch  24/25 Batch   25/276 - Loss:  1.843, Seconds: 0.65\n",
            "Epoch  24/25 Batch   30/276 - Loss:  1.768, Seconds: 0.62\n",
            "Epoch  24/25 Batch   35/276 - Loss:  1.789, Seconds: 0.63\n",
            "Epoch  24/25 Batch   40/276 - Loss:  1.739, Seconds: 0.66\n",
            "Epoch  24/25 Batch   45/276 - Loss:  1.679, Seconds: 0.69\n",
            "Epoch  24/25 Batch   50/276 - Loss:  1.774, Seconds: 0.65\n",
            "Epoch  24/25 Batch   55/276 - Loss:  1.780, Seconds: 0.71\n",
            "Epoch  24/25 Batch   60/276 - Loss:  1.643, Seconds: 0.63\n",
            "Epoch  24/25 Batch   65/276 - Loss:  1.647, Seconds: 0.66\n",
            "Epoch  24/25 Batch   70/276 - Loss:  1.741, Seconds: 0.67\n",
            "Epoch  24/25 Batch   75/276 - Loss:  1.775, Seconds: 0.66\n",
            "Epoch  24/25 Batch   80/276 - Loss:  1.689, Seconds: 0.70\n",
            "Epoch  24/25 Batch   85/276 - Loss:  1.697, Seconds: 0.66\n",
            "Epoch  24/25 Batch   90/276 - Loss:  1.635, Seconds: 0.65\n",
            "Average loss for this update: 1.779\n",
            "No Improvement.\n",
            "Epoch  24/25 Batch   95/276 - Loss:  1.720, Seconds: 0.67\n",
            "Epoch  24/25 Batch  100/276 - Loss:  1.663, Seconds: 0.69\n",
            "Epoch  24/25 Batch  105/276 - Loss:  1.658, Seconds: 0.66\n",
            "Epoch  24/25 Batch  110/276 - Loss:  1.663, Seconds: 0.65\n",
            "Epoch  24/25 Batch  115/276 - Loss:  1.609, Seconds: 0.68\n",
            "Epoch  24/25 Batch  120/276 - Loss:  1.654, Seconds: 0.71\n",
            "Epoch  24/25 Batch  125/276 - Loss:  1.666, Seconds: 0.69\n",
            "Epoch  24/25 Batch  130/276 - Loss:  1.676, Seconds: 0.69\n",
            "Epoch  24/25 Batch  135/276 - Loss:  1.639, Seconds: 0.69\n",
            "Epoch  24/25 Batch  140/276 - Loss:  1.581, Seconds: 0.67\n",
            "Epoch  24/25 Batch  145/276 - Loss:  1.643, Seconds: 0.68\n",
            "Epoch  24/25 Batch  150/276 - Loss:  1.749, Seconds: 0.72\n",
            "Epoch  24/25 Batch  155/276 - Loss:  1.672, Seconds: 0.68\n",
            "Epoch  24/25 Batch  160/276 - Loss:  1.683, Seconds: 0.75\n",
            "Epoch  24/25 Batch  165/276 - Loss:  1.612, Seconds: 0.74\n",
            "Epoch  24/25 Batch  170/276 - Loss:  1.605, Seconds: 0.73\n",
            "Epoch  24/25 Batch  175/276 - Loss:  1.643, Seconds: 0.69\n",
            "Epoch  24/25 Batch  180/276 - Loss:  1.668, Seconds: 0.70\n",
            "Average loss for this update: 1.656\n",
            "New Record!\n",
            "Epoch  24/25 Batch  185/276 - Loss:  1.625, Seconds: 0.76\n",
            "Epoch  24/25 Batch  190/276 - Loss:  1.641, Seconds: 0.66\n",
            "Epoch  24/25 Batch  195/276 - Loss:  1.607, Seconds: 0.71\n",
            "Epoch  24/25 Batch  200/276 - Loss:  1.623, Seconds: 0.71\n",
            "Epoch  24/25 Batch  205/276 - Loss:  1.730, Seconds: 0.71\n",
            "Epoch  24/25 Batch  210/276 - Loss:  1.710, Seconds: 0.76\n",
            "Epoch  24/25 Batch  215/276 - Loss:  1.643, Seconds: 0.72\n",
            "Epoch  24/25 Batch  220/276 - Loss:  1.684, Seconds: 0.72\n",
            "Epoch  24/25 Batch  225/276 - Loss:  1.640, Seconds: 0.84\n",
            "Epoch  24/25 Batch  230/276 - Loss:  1.570, Seconds: 0.74\n",
            "Epoch  24/25 Batch  235/276 - Loss:  1.675, Seconds: 0.74\n",
            "Epoch  24/25 Batch  240/276 - Loss:  1.610, Seconds: 0.72\n",
            "Epoch  24/25 Batch  245/276 - Loss:  1.650, Seconds: 0.75\n",
            "Epoch  24/25 Batch  250/276 - Loss:  1.634, Seconds: 0.76\n",
            "Epoch  24/25 Batch  255/276 - Loss:  1.680, Seconds: 0.75\n",
            "Epoch  24/25 Batch  260/276 - Loss:  1.632, Seconds: 0.77\n",
            "Epoch  24/25 Batch  265/276 - Loss:  1.708, Seconds: 0.77\n",
            "Epoch  24/25 Batch  270/276 - Loss:  1.715, Seconds: 0.79\n",
            "Average loss for this update: 1.655\n",
            "New Record!\n",
            "Epoch  24/25 Batch  275/276 - Loss:  1.706, Seconds: 0.88\n",
            "Epoch  25/25 Batch    5/276 - Loss:  2.358, Seconds: 0.57\n",
            "Epoch  25/25 Batch   10/276 - Loss:  1.824, Seconds: 0.60\n",
            "Epoch  25/25 Batch   15/276 - Loss:  1.837, Seconds: 0.60\n",
            "Epoch  25/25 Batch   20/276 - Loss:  1.831, Seconds: 0.60\n",
            "Epoch  25/25 Batch   25/276 - Loss:  1.791, Seconds: 0.60\n",
            "Epoch  25/25 Batch   30/276 - Loss:  1.743, Seconds: 0.63\n",
            "Epoch  25/25 Batch   35/276 - Loss:  1.784, Seconds: 0.65\n",
            "Epoch  25/25 Batch   40/276 - Loss:  1.714, Seconds: 0.69\n",
            "Epoch  25/25 Batch   45/276 - Loss:  1.614, Seconds: 0.62\n",
            "Epoch  25/25 Batch   50/276 - Loss:  1.696, Seconds: 0.63\n",
            "Epoch  25/25 Batch   55/276 - Loss:  1.704, Seconds: 0.62\n",
            "Epoch  25/25 Batch   60/276 - Loss:  1.661, Seconds: 0.63\n",
            "Epoch  25/25 Batch   65/276 - Loss:  1.618, Seconds: 0.66\n",
            "Epoch  25/25 Batch   70/276 - Loss:  1.735, Seconds: 0.63\n",
            "Epoch  25/25 Batch   75/276 - Loss:  1.718, Seconds: 0.65\n",
            "Epoch  25/25 Batch   80/276 - Loss:  1.696, Seconds: 0.64\n",
            "Epoch  25/25 Batch   85/276 - Loss:  1.667, Seconds: 0.64\n",
            "Epoch  25/25 Batch   90/276 - Loss:  1.607, Seconds: 0.71\n",
            "Average loss for this update: 1.753\n",
            "No Improvement.\n",
            "Epoch  25/25 Batch   95/276 - Loss:  1.674, Seconds: 0.68\n",
            "Epoch  25/25 Batch  100/276 - Loss:  1.656, Seconds: 0.65\n",
            "Epoch  25/25 Batch  105/276 - Loss:  1.627, Seconds: 0.71\n",
            "Epoch  25/25 Batch  110/276 - Loss:  1.622, Seconds: 0.65\n",
            "Epoch  25/25 Batch  115/276 - Loss:  1.603, Seconds: 0.66\n",
            "Epoch  25/25 Batch  120/276 - Loss:  1.665, Seconds: 0.71\n",
            "Epoch  25/25 Batch  125/276 - Loss:  1.660, Seconds: 0.70\n",
            "Epoch  25/25 Batch  130/276 - Loss:  1.640, Seconds: 0.67\n",
            "Epoch  25/25 Batch  135/276 - Loss:  1.580, Seconds: 0.72\n",
            "Epoch  25/25 Batch  140/276 - Loss:  1.526, Seconds: 0.67\n",
            "Epoch  25/25 Batch  145/276 - Loss:  1.618, Seconds: 0.69\n",
            "Epoch  25/25 Batch  150/276 - Loss:  1.717, Seconds: 0.68\n",
            "Epoch  25/25 Batch  155/276 - Loss:  1.657, Seconds: 0.68\n",
            "Epoch  25/25 Batch  160/276 - Loss:  1.645, Seconds: 0.69\n",
            "Epoch  25/25 Batch  165/276 - Loss:  1.616, Seconds: 0.70\n",
            "Epoch  25/25 Batch  170/276 - Loss:  1.597, Seconds: 0.69\n",
            "Epoch  25/25 Batch  175/276 - Loss:  1.622, Seconds: 0.70\n",
            "Epoch  25/25 Batch  180/276 - Loss:  1.651, Seconds: 0.70\n",
            "Average loss for this update: 1.634\n",
            "New Record!\n",
            "Epoch  25/25 Batch  185/276 - Loss:  1.637, Seconds: 0.73\n",
            "Epoch  25/25 Batch  190/276 - Loss:  1.634, Seconds: 0.69\n",
            "Epoch  25/25 Batch  195/276 - Loss:  1.639, Seconds: 0.72\n",
            "Epoch  25/25 Batch  200/276 - Loss:  1.661, Seconds: 0.71\n",
            "Epoch  25/25 Batch  205/276 - Loss:  1.683, Seconds: 0.71\n",
            "Epoch  25/25 Batch  210/276 - Loss:  1.640, Seconds: 0.71\n",
            "Epoch  25/25 Batch  215/276 - Loss:  1.619, Seconds: 0.79\n",
            "Epoch  25/25 Batch  220/276 - Loss:  1.644, Seconds: 0.72\n",
            "Epoch  25/25 Batch  225/276 - Loss:  1.630, Seconds: 0.73\n",
            "Epoch  25/25 Batch  230/276 - Loss:  1.560, Seconds: 0.76\n",
            "Epoch  25/25 Batch  235/276 - Loss:  1.671, Seconds: 0.80\n",
            "Epoch  25/25 Batch  240/276 - Loss:  1.596, Seconds: 0.75\n",
            "Epoch  25/25 Batch  245/276 - Loss:  1.563, Seconds: 0.75\n",
            "Epoch  25/25 Batch  250/276 - Loss:  1.604, Seconds: 0.77\n",
            "Epoch  25/25 Batch  255/276 - Loss:  1.639, Seconds: 0.82\n",
            "Epoch  25/25 Batch  260/276 - Loss:  1.582, Seconds: 0.76\n",
            "Epoch  25/25 Batch  265/276 - Loss:  1.679, Seconds: 0.79\n",
            "Epoch  25/25 Batch  270/276 - Loss:  1.629, Seconds: 0.85\n",
            "Average loss for this update: 1.627\n",
            "New Record!\n",
            "Epoch  25/25 Batch  275/276 - Loss:  1.694, Seconds: 0.92\n",
            "Model Trained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb2odZlehUZt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "67d069fb-672d-48fb-c377-4c1e7b7c2625"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(summary_update_loss[:])\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egtDuCl9y7Uq"
      },
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text, remove_stopwords=True)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mDYSEf7sEbG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdf4404d-9625-41fc-f050-b63eca18a607"
      },
      "source": [
        "# Create your own review or use one from the dataset\n",
        "#input_sentence = \"CS?AVA ? was ?on a training flight with one student and one senior instructor?, who had ?high experience and thousands of hours of flying time?.The dead girl was with her parents, who were unhurt, witnesses told local media. The plane passed over the dead man?s legs as he sunbathed on a towel, they said.One witness, Rita Rogado, 20, told the Público newspaper that the plane crashed right by the sea, dragging her along for a few metres in the sand. Some sunbathers said the plane appeared to be gliding, as they heard no engine noise.? This article was amended on 3 August 2017. An earlier version said São João da Caparica beach was 20 miles south of Lisbon. This has been corrected to 12 miles.\"\n",
        "#text = text_to_seq(input_sentence)\n",
        "count = 0\n",
        "while (count < 5):     \n",
        "   \n",
        "  random = np.random.randint(0,len(clean_texts))\n",
        "  input_sentence = clean_texts[random]\n",
        "  text = text_to_seq(clean_texts[random])\n",
        "\n",
        "  checkpoint = \"./best_model.ckpt\"\n",
        "\n",
        "  loaded_graph = tf.Graph()\n",
        "  with tf.Session(graph=loaded_graph) as sess:\n",
        "      # Load saved model\n",
        "      loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "      loader.restore(sess, checkpoint)\n",
        "\n",
        "      input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "      logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "      text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "      summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "      keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "      \n",
        "      #Multiply by batch_size to match the model's input parameters\n",
        "      answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                        summary_length: [np.random.randint(5,8)], \n",
        "                                        text_length: [len(text)]*batch_size,\n",
        "                                        keep_prob: 1.0})[0] \n",
        "\n",
        "  # Remove the padding from the tweet\n",
        "  pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "  print('Original Text:', mail.text[random])\n",
        "  ogsum = mail.headlines[random]\n",
        "  print('Original summary:',ogsum )#clean_summaries[random]\n",
        "\n",
        "  print('\\nText')\n",
        "  print('  Word Ids:    {}'.format([i for i in text]))\n",
        "  print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "  print('\\nSummary')\n",
        "  print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "  predsum = \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "  print('  Response Words: {}',predsum)\n",
        "  print('\\n\\n')\n",
        "  count = count + 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "Original Text: Tesla and SpaceX CEO Elon Musk was reportedly ready to sell his own stake in SpaceX to take Tesla private. He thought he had a verbal agreement with the Saudi sovereign-wealth fund when he tweeted his plans to take Tesla private. Musk was sued by The US Securities and Exchange Commission for fraud over his \"false and misleading tweets\". \n",
            "Original summary: Musk was ready to sell SpaceX stake to take Tesla private: Report\n",
            "\n",
            "Text\n",
            "  Word Ids:    [963, 3274, 478, 1885, 1886, 13904, 3031, 133, 1458, 3274, 578, 963, 729, 2066, 9179, 2658, 203, 15435, 1511, 399, 11147, 3783, 578, 963, 729, 1886, 2601, 135, 15046, 4424, 3051, 1414, 5073, 1783, 1925]\n",
            "  Input Words: tesla spacex ceo elon musk reportedly ready sell stake spacex take tesla private thought verbal agreement saudi sovereign wealth fund tweeted plans take tesla private musk sued us securities exchange commission fraud false misleading tweets\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [1886, 1886, 3031, 3, 578]\n",
            "  Response Words: {} musk musk ready to take\n",
            "\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "Original Text: Claiming nobody is opposing the construction of a Ram Mandir in Ayodhya, Yoga guru Baba Ramdev on Sunday said, \"People are losing their patience due to the delay on the issue of Ram Temple by the Supreme Court.\" He added, \"In a democracy, the Parliament is supreme. An ordinance should be brought for the construction of the Ram Temple.\"\n",
            "Original summary: People losing patience due to SC delay: Ramdev on Ram Mandir\n",
            "\n",
            "Text\n",
            "  Word Ids:    [4658, 1513, 4706, 2200, 116, 1433, 1610, 8373, 8374, 8360, 123, 5320, 725, 230, 322, 7119, 1476, 5252, 2395, 116, 907, 1852, 1286, 7634, 5961, 5287, 1852, 1432, 2996, 2200, 116, 907]\n",
            "  Input Words: claiming nobody opposing construction ram mandir ayodhya yoga guru baba ramdev sunday said people losing patience due delay issue ram temple supreme court added democracy parliament supreme ordinance brought construction ram temple\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [230, 322, 7119, 5252, 5252]\n",
            "  Response Words: {} people losing patience delay delay\n",
            "\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "Original Text: Union Minister of State for Social Justice and Empowerment Ramdas Athawale said the use of term 'Dalit' should be continued. He added, \"The terms Scheduled Caste and Scheduled Tribe are already in use in government records.\" He further said his party RPI (A) will approach the Supreme Court to challenge the I&B advisory to replace the term with Scheduled Caste. \n",
            "Original summary: Use of term 'Dalit' should continue: Union Minister\n",
            "\n",
            "Text\n",
            "  Word Ids:    [184, 185, 1223, 641, 4768, 4254, 32853, 32853, 725, 594, 766, 7707, 8124, 7634, 5932, 10638, 6189, 10638, 10766, 4252, 594, 1078, 3955, 725, 3463, 28996, 4358, 1852, 1286, 2586, 1798, 7062, 1518, 766, 10638, 6189]\n",
            "  Input Words: union minister state social justice empowerment <UNK> <UNK> said use term dalit continued added terms scheduled caste scheduled tribe already use government records said party rpi approach supreme court challenge b advisory replace term scheduled caste\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [115, 4908, 9643, 99, 72]\n",
            "  Response Words: {} schools authority cites up govt\n",
            "\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "Original Text: Former India captain Sunil Gavaskar has said suspension of Hardik Pandya and KL Rahul and the drama that it caused has taken the sheen off the huge achievement of India's first ever Test series victory in Australia. \"Instead of Indian team basking in the glory of their superb efforts...they have been pushed into the background,\" he added.\n",
            "Original summary: Pandya-Rahul drama spoiled India's Test series victory: Gavaskar \n",
            "\n",
            "Text\n",
            "  Word Ids:    [1315, 30, 1296, 1686, 1687, 725, 1017, 1683, 1587, 2073, 169, 2107, 2765, 941, 20030, 5669, 6901, 30, 342, 1500, 227, 1822, 2804, 1920, 265, 211, 1162, 18646, 18405, 6453, 1846, 751, 7162, 7634]\n",
            "  Input Words: former india captain sunil gavaskar said suspension hardik pandya kl rahul drama caused taken sheen huge achievement india first ever test series victory australia instead indian team basking glory superb efforts pushed background added\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [1687, 1402, 455, 1822, 2804]\n",
            "  Response Words: {} gavaskar manjrekar escape series victory\n",
            "\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "Original Text: In an apparent reference to the surveillance notification, Congress spokesperson Abhishek Singhvi said, \"The ultimate ambition of PM (Narendra) Modi and the government is to become a peeping Tom and turn India into a nanny state.\" Singhvi made the statement after the government authorised 10 agencies to access and decrypt \"information generated, transmitted, received or stored in any computer\".\n",
            "Original summary: Govt wants to be peeping Tom: Congress on surveillance order\n",
            "\n",
            "Text\n",
            "  Word Ids:    [9278, 4745, 5744, 6928, 174, 6636, 4205, 11732, 725, 9821, 19256, 84, 3554, 85, 1078, 639, 8473, 1179, 160, 30, 10891, 1223, 11732, 425, 6580, 1078, 16353, 87, 4020, 812, 15502, 5810, 12581, 22261, 1807, 15963, 2936]\n",
            "  Input Words: apparent reference surveillance notification congress spokesperson abhishek singhvi said ultimate ambition pm narendra modi government become peeping tom turn india nanny state singhvi made statement government authorised 10 agencies access decrypt information generated transmitted received stored computer\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [72, 72, 3, 890, 8473, 8, 30]\n",
            "  Response Words: {} govt govt to be peeping with india\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK_XrRfoe0xu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "af6decca-9a14-45f4-c168-2bae0016b76d"
      },
      "source": [
        "pip install pyrouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyrouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 40kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp36-none-any.whl size=191613 sha256=a965206d03d0f0f79defde1fc68bcc9b7b0144f6d099e89d33030bd06325ea91\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvm3GlQne7pr"
      },
      "source": [
        "from __future__ import division\n",
        "from itertools import chain\n",
        "\n",
        "\n",
        "def get_unigram_count(tokens):\n",
        "    count_dict = dict()\n",
        "    for t in tokens:\n",
        "        if t in count_dict:\n",
        "            count_dict[t] += 1\n",
        "        else:\n",
        "            count_dict[t] = 1\n",
        "\n",
        "    return count_dict\n",
        "\n",
        "\n",
        "class Rouge:\n",
        "    beta = 1\n",
        "\n",
        "    @staticmethod\n",
        "    def my_lcs_grid(x, y):\n",
        "        n = len(x)\n",
        "        m = len(y)\n",
        "\n",
        "        table = [[0 for i in range(m + 1)] for j in range(n + 1)]\n",
        "\n",
        "        for j in range(m + 1):\n",
        "            for i in range(n + 1):\n",
        "                if i == 0 or j == 0:\n",
        "                    cell = (0, 'e')\n",
        "                elif x[i - 1] == y[j - 1]:\n",
        "                    cell = (table[i - 1][j - 1][0] + 1, '\\\\')\n",
        "                else:\n",
        "                    over = table[i - 1][j][0]\n",
        "                    left = table[i][j - 1][0]\n",
        "\n",
        "                    if left < over:\n",
        "                        cell = (over, '^')\n",
        "                    else:\n",
        "                        cell = (left, '<')\n",
        "\n",
        "                table[i][j] = cell\n",
        "\n",
        "        return table\n",
        "\n",
        "    @staticmethod\n",
        "    def my_lcs(x, y, mask_x):\n",
        "        table = Rouge.my_lcs_grid(x, y)\n",
        "        i = len(x)\n",
        "        j = len(y)\n",
        "\n",
        "        while i > 0 and j > 0:\n",
        "            move = table[i][j][1]\n",
        "            if move == '\\\\':\n",
        "                mask_x[i - 1] = 1\n",
        "                i -= 1\n",
        "                j -= 1\n",
        "            elif move == '^':\n",
        "                i -= 1\n",
        "            elif move == '<':\n",
        "                j -= 1\n",
        "\n",
        "        return mask_x\n",
        "\n",
        "    @staticmethod\n",
        "    def rouge_l(cand_sents, ref_sents):\n",
        "        lcs_scores = 0.0\n",
        "        cand_unigrams = get_unigram_count(chain(*cand_sents))\n",
        "        ref_unigrams = get_unigram_count(chain(*ref_sents))\n",
        "        for cand_sent in cand_sents:\n",
        "            cand_token_mask = [0 for t in cand_sent]\n",
        "            cand_len = len(cand_sent)\n",
        "            for ref_sent in ref_sents:\n",
        "                # aligns = []\n",
        "                # Rouge.lcs(ref_sent, cand_sent, aligns)\n",
        "                Rouge.my_lcs(cand_sent, ref_sent, cand_token_mask)\n",
        "\n",
        "                # for i in aligns:\n",
        "                #     ref_token_mask[i] = 1\n",
        "            # lcs = []\n",
        "            cur_lcs_score = 0.0\n",
        "            for i in range(cand_len):\n",
        "                if cand_token_mask[i]:\n",
        "                    token = cand_sent[i]\n",
        "                    if cand_unigrams[token] > 0 and ref_unigrams[token] > 0:\n",
        "                        cand_unigrams[token] -= 1\n",
        "                        ref_unigrams[token] -= 1\n",
        "                        cur_lcs_score += 1\n",
        "\n",
        "                        # lcs.append(token)\n",
        "\n",
        "            # print ' '.join(lcs)\n",
        "\n",
        "            lcs_scores += cur_lcs_score\n",
        "\n",
        "        # print \"lcs_scores: %d\" % lcs_scores\n",
        "        ref_words_count = sum(len(s) for s in ref_sents)\n",
        "        # print \"ref_words_count: %d\" % ref_words_count\n",
        "        cand_words_count = sum(len(s) for s in cand_sents)\n",
        "        # print \"cand_words_count: %d\" % cand_words_count\n",
        "\n",
        "        precision = lcs_scores / cand_words_count\n",
        "        recall = lcs_scores / ref_words_count\n",
        "        f_score = (1 + Rouge.beta ** 2) * precision * recall / (recall +\n",
        "                                                                Rouge.beta ** 2 * precision + 1e-7) + 1e-6  # prevent underflow\n",
        "        return precision, recall, f_score\n",
        "\n",
        "    # @staticmethod\n",
        "    # def rouge_2(cand_sents, ref_sents):\n",
        "    #     cand_bigram_counts = get_bigram_counts(cand_sents)\n",
        "    #     ref_bigram_counts = get_bigram_counts(ref_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkMpnoNcfUad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c33f7a28-9ec6-47b3-bc5c-0c2ebda858d8"
      },
      "source": [
        "r = Rouge()\n",
        "\n",
        "system_generated_summary = ogsum\n",
        "manual_summmary = predsum\n",
        "\n",
        "[precision, recall, f_score] = r.rouge_l([system_generated_summary], [manual_summmary])\n",
        "\n",
        "print(\"Precision is :\"+str(precision)+\"\\nRecall is :\"+str(recall)+\"\\nF Score is :\"+str(f_score))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision is :0.4\n",
            "Recall is :0.7058823529411765\n",
            "F Score is :0.5106392516976049\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}